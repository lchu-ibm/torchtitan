W0722 18:58:23.822000 22864786958144 torch/distributed/run.py:793] 
W0722 18:58:23.822000 22864786958144 torch/distributed/run.py:793] *****************************************
W0722 18:58:23.822000 22864786958144 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 18:58:23.822000 22864786958144 torch/distributed/run.py:793] *****************************************
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:29,258 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:58:37,765 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:37,774 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:37,820 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:38,032 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:38,032 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:38,972 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:38,975 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:38,989 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,171 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,171 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:39,315 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:39,318 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:39,333 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,455 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:39,458 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:39,473 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,494 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:39,506 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:39,512 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:39,515 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,515 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:39,521 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:39,524 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,529 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,605 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:39,615 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:58:39,616 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:39,620 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:58:39,621 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,624 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:58:39,656 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,656 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:39,716 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,716 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:39,720 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,721 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:39,819 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,819 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:58:39,823 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:58:39,823 - root - INFO - Preparing c4 dataset from allenai/c4
Downloading readme:   0%|          | 0.00/41.1k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 41.1k/41.1k [00:00<00:00, 2.65MB/s]
2024-07-22 18:58:43,999 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:44,178 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:44,179 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 18:58:44,179 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:44,252 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:47,556 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:47,620 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:47,728 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:47,729 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 18:58:47,729 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:47,793 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:47,794 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 18:58:47,795 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:47,806 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:47,861 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:48,572 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:48,757 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:48,757 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 18:58:48,758 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:48,826 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:48,943 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:49,115 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:49,116 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 18:58:49,117 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:49,158 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:49,182 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:49,330 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:49,331 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 18:58:49,331 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:49,409 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:50,497 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:50,669 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:50,669 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 18:58:50,670 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:50,736 - root - INFO - Applied FSDP to the model
2024-07-22 18:58:50,786 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:58:50,960 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:58:50,962 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 18:58:50,962 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:58:51,028 - root - INFO - Applied FSDP to the model
2024-07-22 18:59:02,663 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,663 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,663 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,663 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,664 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,664 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,664 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,665 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,665 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,666 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1859
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,667 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,667 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 18:59:02,675 - root - INFO - Training starts at step 1
2024-07-22 18:59:02,675 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank4]:     pred = model(input_ids)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank4]:     h = layer(h, self.freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank4]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank4]:     ret = function(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank4]:     raise NotImplementedError(
[rank4]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank2]:     pred = model(input_ids)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank2]:     h = layer(h, self.freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank2]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank2]:     ret = function(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank2]:     raise NotImplementedError(
[rank2]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank6]:     pred = model(input_ids)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank6]:     h = layer(h, self.freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank6]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank6]:     ret = function(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank6]:     raise NotImplementedError(
[rank6]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank1]:     pred = model(input_ids)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank1]:     h = layer(h, self.freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank1]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank1]:     ret = function(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank1]:     raise NotImplementedError(
[rank1]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank0]:     pred = model(input_ids)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank0]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank0]:     ret = function(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank0]:     raise NotImplementedError(
[rank0]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank5]:     pred = model(input_ids)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank5]:     h = layer(h, self.freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank5]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank5]:     ret = function(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank5]:     raise NotImplementedError(
[rank5]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank7]:     pred = model(input_ids)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank7]:     h = layer(h, self.freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank7]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank7]:     ret = function(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank7]:     raise NotImplementedError(
[rank7]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank3]:     pred = model(input_ids)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank3]:     h = layer(h, self.freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank3]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank3]:     ret = function(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
[rank3]:     raise NotImplementedError(
[rank3]: NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
W0722 18:59:06.696000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583226 closing signal SIGTERM
W0722 18:59:06.702000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583227 closing signal SIGTERM
W0722 18:59:06.702000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583228 closing signal SIGTERM
W0722 18:59:06.702000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583229 closing signal SIGTERM
W0722 18:59:06.702000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583230 closing signal SIGTERM
W0722 18:59:06.702000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583231 closing signal SIGTERM
W0722 18:59:06.702000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3583232 closing signal SIGTERM
E0722 18:59:08.017000 22864786958144 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 0 (pid: 3583225) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 18:59:08.025000 22864786958144 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_yoxu11kl/none_ixv59uy1/attempt_0/0/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_18:59:05
  host      : p5-r11-n3.bluevela.rmf.ibm.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3583225)
  error_file: /tmp/torchelastic_yoxu11kl/none_ixv59uy1/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 393, in _forward_unimplemented
      raise NotImplementedError(
  NotImplementedError: Module [TransformerBlock] is missing the required "forward" function
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p5-r11-n3>
Subject: Job 107673: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 18:58:17 2024
Job was executed on host(s) <p5-r11-n3>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 18:58:17 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 18:58:17 2024
Terminated at Mon Jul 22 18:59:09 2024
Results reported at Mon Jul 22 18:59:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   221.00 sec.
    Max Memory :                                 10377 MB
    Average Memory :                             3802.50 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   59 sec.
    Turnaround time :                            52 sec.

The output (if any) is above this job summary.

W0722 18:59:37.469000 22396466763584 torch/distributed/run.py:793] 
W0722 18:59:37.469000 22396466763584 torch/distributed/run.py:793] *****************************************
W0722 18:59:37.469000 22396466763584 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 18:59:37.469000 22396466763584 torch/distributed/run.py:793] *****************************************
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,762 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:42,770 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 18:59:50,700 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:50,704 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:50,727 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:50,910 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:50,910 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:51,709 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:51,712 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:51,726 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:51,910 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:51,910 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:52,323 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:52,326 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:52,340 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:52,388 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:52,391 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:52,396 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:52,397 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:52,398 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:52,400 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:52,401 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:52,403 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:52,405 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:52,405 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:52,408 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:52,413 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:52,461 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 18:59:52,463 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 18:59:52,466 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 18:59:52,528 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:52,528 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:52,595 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:52,595 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:52,596 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:52,596 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:52,597 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:52,597 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:52,606 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:52,606 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:52,657 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 18:59:52,657 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 18:59:57,930 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:59:58,104 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:59:58,104 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 18:59:58,105 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:59:58,183 - root - INFO - Applied FSDP to the model
2024-07-22 18:59:58,474 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:59:58,646 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:59:58,647 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 18:59:58,648 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:59:58,714 - root - INFO - Applied FSDP to the model
2024-07-22 18:59:58,752 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:59:58,929 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:59:58,930 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 18:59:58,931 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:59:58,999 - root - INFO - Applied FSDP to the model
2024-07-22 18:59:59,074 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:59:59,245 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:59:59,246 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 18:59:59,247 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:59:59,313 - root - INFO - Applied FSDP to the model
2024-07-22 18:59:59,318 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 18:59:59,490 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 18:59:59,491 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 18:59:59,492 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 18:59:59,561 - root - INFO - Applied FSDP to the model
2024-07-22 19:00:00,081 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:00:00,253 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:00:00,253 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 19:00:00,254 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:00:00,320 - root - INFO - Applied FSDP to the model
2024-07-22 19:00:00,361 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:00:00,421 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:00:00,535 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:00:00,535 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 19:00:00,536 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:00:00,593 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:00:00,594 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 19:00:00,594 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:00:00,602 - root - INFO - Applied FSDP to the model
2024-07-22 19:00:00,661 - root - INFO - Applied FSDP to the model
2024-07-22 19:00:13,771 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,772 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,772 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,772 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,772 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,772 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,772 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,773 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,775 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1900
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,777 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:00:13,785 - root - INFO - Training starts at step 1
2024-07-22 19:00:13,785 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank1]:     pred = model(input_ids)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank1]:     h = layer(h, self.freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank1]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank1]:     ret = function(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank1]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank1]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank1]: AttributeError: 'NoneType' object has no attribute 'view'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank0]:     pred = model(input_ids)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank0]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank0]:     ret = function(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank0]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank0]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank0]: AttributeError: 'NoneType' object has no attribute 'view'
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank5]:     pred = model(input_ids)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank5]:     h = layer(h, self.freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank5]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank5]:     ret = function(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank5]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank5]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank5]: AttributeError: 'NoneType' object has no attribute 'view'
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank7]:     pred = model(input_ids)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank7]:     h = layer(h, self.freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank7]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank7]:     ret = function(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank7]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank7]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank7]: AttributeError: 'NoneType' object has no attribute 'view'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank2]:     pred = model(input_ids)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank2]:     h = layer(h, self.freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank2]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank2]:     ret = function(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank2]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank2]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank2]: AttributeError: 'NoneType' object has no attribute 'view'
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank3]:     pred = model(input_ids)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank3]:     h = layer(h, self.freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank3]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank3]:     ret = function(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank3]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank3]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank3]: AttributeError: 'NoneType' object has no attribute 'view'
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank6]:     pred = model(input_ids)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank6]:     h = layer(h, self.freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank6]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank6]:     ret = function(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank6]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank6]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank6]: AttributeError: 'NoneType' object has no attribute 'view'
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank4]:     pred = model(input_ids)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
[rank4]:     h = layer(h, self.freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank4]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank4]:     ret = function(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank4]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
[rank4]:     xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
[rank4]: AttributeError: 'NoneType' object has no attribute 'view'
W0722 19:00:17.741000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492826 closing signal SIGTERM
W0722 19:00:17.741000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492827 closing signal SIGTERM
W0722 19:00:17.741000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492828 closing signal SIGTERM
W0722 19:00:17.741000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492829 closing signal SIGTERM
W0722 19:00:17.742000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492831 closing signal SIGTERM
W0722 19:00:17.742000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492832 closing signal SIGTERM
W0722 19:00:17.742000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3492833 closing signal SIGTERM
E0722 19:00:19.107000 22396466763584 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 4 (pid: 3492830) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 19:00:19.115000 22396466763584 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_65zy6ppi/none_9rfou_if/attempt_0/4/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_19:00:16
  host      : p5-r16-n3.bluevela.rmf.ibm.com
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 3492830)
  error_file: /tmp/torchelastic_65zy6ppi/none_9rfou_if/attempt_0/4/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 439, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
      h = x + self.attention(self.attention_norm(x), freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 199, in forward
      xk = xk.view(bs, seqlen, self.n_kv_heads, self.head_dim)
  AttributeError: 'NoneType' object has no attribute 'view'
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p5-r16-n3>
Subject: Job 107675: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 18:59:29 2024
Job was executed on host(s) <p5-r16-n3>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 18:59:31 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 18:59:31 2024
Terminated at Mon Jul 22 19:00:20 2024
Results reported at Mon Jul 22 19:00:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   204.00 sec.
    Max Memory :                                 11225 MB
    Average Memory :                             6866.62 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   49 sec.
    Turnaround time :                            51 sec.

The output (if any) is above this job summary.

W0722 19:02:24.115000 22689538111296 torch/distributed/run.py:793] 
W0722 19:02:24.115000 22689538111296 torch/distributed/run.py:793] *****************************************
W0722 19:02:24.115000 22689538111296 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 19:02:24.115000 22689538111296 torch/distributed/run.py:793] *****************************************
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,269 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:29,274 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:02:39,586 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:39,591 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:39,633 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:39,847 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:39,847 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:39,912 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:39,920 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:39,940 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:39,941 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:39,943 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:39,945 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:40,127 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,127 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:40,131 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,131 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:40,208 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:40,212 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:40,228 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:40,273 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:40,276 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:40,277 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:40,280 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:40,281 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:40,282 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:40,286 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:40,288 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:02:40,293 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:40,293 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:02:40,297 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:40,301 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:02:40,412 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,412 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:40,475 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,475 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:40,478 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,479 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:40,488 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,488 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:40,489 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:02:40,489 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:02:46,083 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:46,263 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:46,263 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 19:02:46,264 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:46,338 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:47,277 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:47,356 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:47,453 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:47,453 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 19:02:47,454 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:47,530 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:47,532 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 19:02:47,532 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:47,536 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:47,600 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:48,027 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:48,070 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:48,206 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:48,207 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 19:02:48,208 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:48,249 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:48,251 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 19:02:48,252 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:48,277 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:48,323 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:48,458 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:48,465 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:48,560 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:02:48,634 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:48,634 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 19:02:48,635 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:48,639 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:48,640 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 19:02:48,641 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:48,703 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:48,713 - root - INFO - Applied FSDP to the model
2024-07-22 19:02:48,734 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:02:48,735 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 19:02:48,735 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:02:48,803 - root - INFO - Applied FSDP to the model
2024-07-22 19:03:01,238 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,239 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,239 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,239 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,239 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,240 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,240 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,240 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,242 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1903
2024-07-22 19:03:01,243 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,243 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,243 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,243 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,244 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,244 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,244 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,244 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:03:01,252 - root - INFO - Training starts at step 1
2024-07-22 19:03:01,252 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank7]:     loss.backward()
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank7]:     frame.recompute_fn(*args)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank7]:     fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank7]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank7]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank7]:     return F.linear(input, self.weight, self.bias)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank7]:     out = func(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank7]:     return self_._op(*args, **kwargs)
[rank7]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank6]:     loss.backward()
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank6]:     frame.recompute_fn(*args)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank6]:     fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank6]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank6]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank6]:     return F.linear(input, self.weight, self.bias)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank6]:     out = func(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank6]:     return self_._op(*args, **kwargs)
[rank6]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank4]:     loss.backward()
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank4]:     frame.recompute_fn(*args)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank4]:     fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank4]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank4]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank4]:     return F.linear(input, self.weight, self.bias)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank4]:     out = func(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank4]:     return self_._op(*args, **kwargs)
[rank4]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank3]:     loss.backward()
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank3]:     frame.recompute_fn(*args)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank3]:     fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank3]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank3]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank3]:     return F.linear(input, self.weight, self.bias)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank3]:     return self_._op(*args, **kwargs)
[rank3]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank1]:     loss.backward()
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank1]:     frame.recompute_fn(*args)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank1]:     fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank1]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank1]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank1]:     return F.linear(input, self.weight, self.bias)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank1]:     return self_._op(*args, **kwargs)
[rank1]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank0]:     loss.backward()
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank0]:     frame.recompute_fn(*args)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank0]:     fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank0]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank0]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank0]:     return self_._op(*args, **kwargs)
[rank0]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank2]:     loss.backward()
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank2]:     frame.recompute_fn(*args)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank2]:     fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank2]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank2]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank2]:     return F.linear(input, self.weight, self.bias)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank2]:     return self_._op(*args, **kwargs)
[rank2]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank5]:     loss.backward()
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank5]:     frame.recompute_fn(*args)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank5]:     fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank5]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank5]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank5]:     return F.linear(input, self.weight, self.bias)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank5]:     out = func(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank5]:     return self_._op(*args, **kwargs)
[rank5]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
W0722 19:03:07.386000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574816 closing signal SIGTERM
W0722 19:03:07.390000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574817 closing signal SIGTERM
W0722 19:03:07.390000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574818 closing signal SIGTERM
W0722 19:03:07.390000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574819 closing signal SIGTERM
W0722 19:03:07.390000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574820 closing signal SIGTERM
W0722 19:03:07.390000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574821 closing signal SIGTERM
W0722 19:03:07.390000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 1574823 closing signal SIGTERM
E0722 19:03:10.284000 22689538111296 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 6 (pid: 1574822) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 19:03:10.292000 22689538111296 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_ipup25vs/none_p0xqsf51/attempt_0/6/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_19:03:05
  host      : p5-r01-n2.bluevela.rmf.ibm.com
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 1574822)
  error_file: /tmp/torchelastic_ipup25vs/none_p0xqsf51/attempt_0/6/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
      loss.backward()
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
      torch.autograd.backward(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
      _engine_run_backward(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
      frame.recompute_fn(*args)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
      fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
      h = x + self.attention(self.attention_norm(x), freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
      xq, xk, xv = self.wq(x), self.xk, self.xv
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
      return F.linear(input, self.weight, self.bias)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p5-r01-n2>
Subject: Job 107677: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 19:02:16 2024
Job was executed on host(s) <p5-r01-n2>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 19:02:18 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 19:02:18 2024
Terminated at Mon Jul 22 19:03:12 2024
Results reported at Mon Jul 22 19:03:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   206.00 sec.
    Max Memory :                                 10816 MB
    Average Memory :                             4395.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   57 sec.
    Turnaround time :                            56 sec.

The output (if any) is above this job summary.

W0722 19:43:35.672000 23203338110784 torch/distributed/run.py:793] 
W0722 19:43:35.672000 23203338110784 torch/distributed/run.py:793] *****************************************
W0722 19:43:35.672000 23203338110784 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 19:43:35.672000 23203338110784 torch/distributed/run.py:793] *****************************************
2024-07-22 19:43:41,924 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,924 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,924 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,924 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,924 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,924 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,925 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:41,925 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:43:51,557 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:51,566 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:51,608 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:51,794 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:51,795 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:51,819 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:51,822 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:51,836 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:51,893 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:51,897 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:51,912 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:52,024 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,024 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:52,100 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,101 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:52,511 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:52,516 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:52,519 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:52,522 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:52,524 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:52,532 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:52,536 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:52,536 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:52,539 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:52,539 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:52,542 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:52,542 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:52,631 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:43:52,641 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:43:52,658 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:43:52,714 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,714 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:52,718 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,719 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:52,730 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,731 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:52,737 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,737 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:52,844 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:43:52,844 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:43:56,731 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:56,911 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:56,911 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 19:43:56,912 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:56,986 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:57,749 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:57,927 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:57,928 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 19:43:57,929 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:57,997 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:58,521 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:58,695 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:58,696 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 19:43:58,696 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:58,763 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:58,832 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:59,015 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:59,017 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 19:43:59,018 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:59,043 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:59,086 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:59,218 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:59,218 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 19:43:59,219 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:59,286 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:59,523 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:59,547 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:43:59,699 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:59,699 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 19:43:59,700 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:59,723 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:43:59,723 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 19:43:59,724 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:43:59,767 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:59,793 - root - INFO - Applied FSDP to the model
2024-07-22 19:43:59,936 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:44:00,111 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:44:00,112 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 19:44:00,113 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:44:00,180 - root - INFO - Applied FSDP to the model
2024-07-22 19:44:12,557 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,558 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,558 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,558 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,559 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,559 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,559 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,559 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,561 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1944
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,569 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:44:12,571 - root - INFO - Training starts at step 1
2024-07-22 19:44:12,572 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank6]:     loss.backward()
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank6]:     frame.recompute_fn(*args)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank6]:     fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank6]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank6]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank6]:     return F.linear(input, self.weight, self.bias)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank6]:     out = func(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank6]:     return self_._op(*args, **kwargs)
[rank6]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank7]:     loss.backward()
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank7]:     frame.recompute_fn(*args)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank7]:     fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank7]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank7]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank7]:     return F.linear(input, self.weight, self.bias)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank7]:     out = func(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank7]:     return self_._op(*args, **kwargs)
[rank7]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank4]:     loss.backward()
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank4]:     frame.recompute_fn(*args)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank4]:     fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank4]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank4]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank4]:     return F.linear(input, self.weight, self.bias)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank4]:     out = func(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank4]:     return self_._op(*args, **kwargs)
[rank4]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank0]:     loss.backward()
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank0]:     frame.recompute_fn(*args)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank0]:     fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank0]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank0]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank0]:     return self_._op(*args, **kwargs)
[rank0]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank3]:     loss.backward()
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank3]:     frame.recompute_fn(*args)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank3]:     fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank3]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank3]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank3]:     return F.linear(input, self.weight, self.bias)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank3]:     return self_._op(*args, **kwargs)
[rank3]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank2]:     loss.backward()
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank2]:     frame.recompute_fn(*args)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank2]:     fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank2]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank2]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank2]:     return F.linear(input, self.weight, self.bias)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank2]:     return self_._op(*args, **kwargs)
[rank2]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank5]:     loss.backward()
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank5]:     frame.recompute_fn(*args)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank5]:     fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank5]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank5]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank5]:     return F.linear(input, self.weight, self.bias)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank5]:     out = func(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank5]:     return self_._op(*args, **kwargs)
[rank5]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
[rank1]:     loss.backward()
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
[rank1]:     frame.recompute_fn(*args)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
[rank1]:     fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
[rank1]:     h = x + self.attention(self.attention_norm(x), freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
[rank1]:     xq, xk, xv = self.wq(x), self.xk, self.xv
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank1]:     return F.linear(input, self.weight, self.bias)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank1]:     return self_._op(*args, **kwargs)
[rank1]: RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
W0722 19:44:18.843000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339895 closing signal SIGTERM
W0722 19:44:18.846000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339896 closing signal SIGTERM
W0722 19:44:18.846000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339897 closing signal SIGTERM
W0722 19:44:18.846000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339898 closing signal SIGTERM
W0722 19:44:18.846000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339899 closing signal SIGTERM
W0722 19:44:18.846000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339900 closing signal SIGTERM
W0722 19:44:18.846000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3339901 closing signal SIGTERM
E0722 19:44:20.575000 23203338110784 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 7 (pid: 3339902) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 19:44:20.583000 23203338110784 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_xkz5lpyl/none_cxkodz58/attempt_0/7/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_19:44:17
  host      : p5-r19-n2.bluevela.rmf.ibm.com
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 3339902)
  error_file: /tmp/torchelastic_xkz5lpyl/none_cxkodz58/attempt_0/7/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 388, in main
      loss.backward()
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
      torch.autograd.backward(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/__init__.py", line 288, in backward
      _engine_run_backward(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py", line 799, in _engine_run_backward
      return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1122, in unpack_hook
      frame.recompute_fn(*args)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1516, in recompute_fn
      fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 328, in forward
      h = x + self.attention(self.attention_norm(x), freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 195, in forward
      xq, xk, xv = self.wq(x), self.xk, self.xv
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
      return F.linear(input, self.weight, self.bias)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1327, in __torch_dispatch__
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  RuntimeError: shape '[1, 8192, 4096]' is invalid for input of size 8388608
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p5-r19-n2>
Subject: Job 107694: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 19:43:27 2024
Job was executed on host(s) <p5-r19-n2>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 19:43:29 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 19:43:29 2024
Terminated at Mon Jul 22 19:44:21 2024
Results reported at Mon Jul 22 19:44:21 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   227.00 sec.
    Max Memory :                                 9943 MB
    Average Memory :                             6036.50 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   53 sec.
    Turnaround time :                            54 sec.

The output (if any) is above this job summary.

W0722 19:54:10.086000 23407044015936 torch/distributed/run.py:793] 
W0722 19:54:10.086000 23407044015936 torch/distributed/run.py:793] *****************************************
W0722 19:54:10.086000 23407044015936 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 19:54:10.086000 23407044015936 torch/distributed/run.py:793] *****************************************
2024-07-22 19:54:18,561 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,561 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,561 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,562 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,562 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,562 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,562 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:18,563 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:54:27,154 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:27,160 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:27,198 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:27,432 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:27,432 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:27,721 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:27,725 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:27,733 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:27,888 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:27,899 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:27,917 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:27,924 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:27,924 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:28,082 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:28,089 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:28,093 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:28,098 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:28,099 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:28,103 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:28,103 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:28,104 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:54:28,105 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:28,106 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:28,107 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:28,108 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:54:28,111 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:28,113 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:28,116 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:28,117 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:28,123 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:54:28,300 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:28,300 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:28,307 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:28,307 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:28,314 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:28,314 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:28,317 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:28,317 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:28,325 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:54:28,325 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:54:33,779 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:33,959 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:33,960 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 19:54:33,961 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:34,046 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:34,604 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:34,776 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:34,776 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 19:54:34,777 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:34,846 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:34,925 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:34,984 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:35,103 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:35,103 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 19:54:35,104 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:35,160 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:35,163 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 19:54:35,164 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:35,174 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:35,230 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:35,643 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:35,723 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:35,772 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:35,803 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:54:35,817 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:35,818 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 19:54:35,819 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:35,884 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:35,895 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:35,895 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 19:54:35,896 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:35,946 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:35,946 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 19:54:35,947 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:35,962 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:35,980 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:54:35,981 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 19:54:35,982 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:54:36,015 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:36,047 - root - INFO - Applied FSDP to the model
2024-07-22 19:54:48,891 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,892 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,892 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,893 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,895 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,895 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,895 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,895 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,897 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,897 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,897 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,897 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,897 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,897 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,905 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,905 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,929 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,930 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,930 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,931 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,931 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:54:48,931 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,932 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,932 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,932 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,932 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1954
2024-07-22 19:54:48,933 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,933 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,933 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,933 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:54:48,934 - root - INFO - Training starts at step 1
2024-07-22 19:54:48,934 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank4]:     pred = model(input_ids)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank4]:     h = layer(h, self.freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank4]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank4]:     ret = function(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank4]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank4]:     return self.wo(output), xk0, xv0
[rank4]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank3]:     pred = model(input_ids)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank3]:     h = layer(h, self.freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank3]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank3]:     ret = function(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank3]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank3]:     return self.wo(output), xk0, xv0
[rank3]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank1]:     pred = model(input_ids)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank1]:     h = layer(h, self.freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank1]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank1]:     ret = function(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank1]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank1]:     return self.wo(output), xk0, xv0
[rank1]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank0]:     pred = model(input_ids)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank0]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank0]:     ret = function(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank0]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank0]:     return self.wo(output), xk0, xv0
[rank0]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank7]:     pred = model(input_ids)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank7]:     h = layer(h, self.freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank7]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank7]:     ret = function(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank7]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank7]:     return self.wo(output), xk0, xv0
[rank7]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank6]:     pred = model(input_ids)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank6]:     h = layer(h, self.freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank6]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank6]:     ret = function(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank6]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank6]:     return self.wo(output), xk0, xv0
[rank6]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank2]:     pred = model(input_ids)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank2]:     h = layer(h, self.freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank2]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank2]:     ret = function(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank2]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank2]:     return self.wo(output), xk0, xv0
[rank2]: UnboundLocalError: local variable 'xk0' referenced before assignment
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank5]:     pred = model(input_ids)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank5]:     h = layer(h, self.freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank5]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank5]:     ret = function(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
[rank5]:     h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
[rank5]:     return self.wo(output), xk0, xv0
[rank5]: UnboundLocalError: local variable 'xk0' referenced before assignment
W0722 19:54:54.459000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968020 closing signal SIGTERM
W0722 19:54:54.464000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968021 closing signal SIGTERM
W0722 19:54:54.464000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968022 closing signal SIGTERM
W0722 19:54:54.464000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968023 closing signal SIGTERM
W0722 19:54:54.464000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968024 closing signal SIGTERM
W0722 19:54:54.464000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968025 closing signal SIGTERM
W0722 19:54:54.464000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3968026 closing signal SIGTERM
E0722 19:54:55.993000 23407044015936 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 0 (pid: 3968019) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 19:54:56.000000 23407044015936 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_jyfbme0h/none_eh9hapjh/attempt_0/0/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_19:54:52
  host      : p3-r22-n4.bluevela.rmf.ibm.com
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3968019)
  error_file: /tmp/torchelastic_jyfbme0h/none_eh9hapjh/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 330, in forward
      h, xk, xv = self.attention(self.attention_norm(x), freqs_cis, xk, xv)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 219, in forward
      return self.wo(output), xk0, xv0
  UnboundLocalError: local variable 'xk0' referenced before assignment
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p3-r22-n4>
Subject: Job 107721: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 19:54:02 2024
Job was executed on host(s) <p3-r22-n4>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 19:54:02 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 19:54:02 2024
Terminated at Mon Jul 22 19:54:57 2024
Results reported at Mon Jul 22 19:54:57 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   207.00 sec.
    Max Memory :                                 9866 MB
    Average Memory :                             6010.50 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   56 sec.
    Turnaround time :                            55 sec.

The output (if any) is above this job summary.

W0722 19:56:01.077000 22829218690880 torch/distributed/run.py:793] 
W0722 19:56:01.077000 22829218690880 torch/distributed/run.py:793] *****************************************
W0722 19:56:01.077000 22829218690880 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 19:56:01.077000 22829218690880 torch/distributed/run.py:793] *****************************************
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,535 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:04,537 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 19:56:14,218 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:14,220 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:14,234 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:14,414 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:14,414 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:14,707 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:14,711 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:14,726 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:14,797 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:14,800 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:14,814 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:14,913 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:14,913 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:14,998 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:14,998 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:15,128 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:15,129 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:15,131 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:15,131 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:15,133 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:15,135 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:15,138 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:15,141 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:15,145 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:15,190 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:15,192 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:15,193 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 19:56:15,194 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:15,196 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 19:56:15,198 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 19:56:15,325 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:15,325 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:15,332 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:15,333 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:15,340 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:15,340 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:15,390 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:15,390 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:15,390 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 19:56:15,390 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 19:56:20,004 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:20,180 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:20,181 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 19:56:20,181 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:20,256 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:21,259 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:21,436 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:21,436 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 19:56:21,437 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:21,505 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:21,802 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:21,853 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:21,973 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:21,974 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 19:56:21,975 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:21,981 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:22,024 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:22,024 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 19:56:22,025 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:22,041 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:22,090 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:22,150 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:22,150 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 19:56:22,151 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:22,217 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:22,980 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:23,156 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:23,156 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 19:56:23,157 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:23,223 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:23,439 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:23,610 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:23,610 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 19:56:23,611 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:23,680 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:24,906 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 19:56:25,079 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 19:56:25,080 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 19:56:25,081 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 19:56:25,146 - root - INFO - Applied FSDP to the model
2024-07-22 19:56:36,908 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,909 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,909 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,909 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,909 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,909 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,910 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,910 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,910 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,910 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 19:56:36,910 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,910 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,911 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,911 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,911 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,911 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,911 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,911 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,912 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,912 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,912 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,912 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-1956
2024-07-22 19:56:36,912 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,912 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,912 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,912 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,913 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,913 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,913 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,913 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 19:56:36,916 - root - INFO - Training starts at step 1
2024-07-22 19:56:36,916 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 817  mfu: 4.79%
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,932 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,932 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,932 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,932 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,932 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,933 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,932 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,933 - root - INFO - step:  1  loss: 12.2504  memory: 54.21GiB(68.53%)  wps: 818  mfu: 4.79%
2024-07-22 19:56:46,933 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:56:46,934 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,622 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:07,623 - root - INFO - step: 10  loss: 10.6701  memory: 58.13GiB(73.48%)  wps: 3,564  mfu: 20.87%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,555  mfu: 20.82%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,555  mfu: 20.81%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,555  mfu: 20.82%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,555  mfu: 20.82%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,556  mfu: 20.82%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,555  mfu: 20.81%
2024-07-22 19:57:30,686 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,556  mfu: 20.83%
2024-07-22 19:57:30,687 - root - INFO - step: 20  loss:  9.0676  memory: 58.13GiB(73.48%)  wps: 3,555  mfu: 20.82%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 19:57:53,838 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 19:57:53,839 - root - INFO - step: 30  loss:  8.0306  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 19:58:16,995 - root - INFO - step: 40  loss:  7.4487  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:58:40,188 - root - INFO - step: 50  loss:  7.1996  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 19:59:03,370 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:03,371 - root - INFO - step: 60  loss:  7.0522  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 19:59:26,560 - root - INFO - step: 70  loss:  6.9258  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 19:59:49,724 - root - INFO - step: 80  loss:  6.7791  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:00:12,877 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:00:12,878 - root - INFO - step: 90  loss:  6.6314  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,449  mfu: 20.20%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,448  mfu: 20.19%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,448  mfu: 20.19%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,450  mfu: 20.20%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,448  mfu: 20.19%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,449  mfu: 20.20%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,450  mfu: 20.20%
2024-07-22 20:00:36,650 - root - INFO - step: 100  loss:  6.5825  memory: 58.13GiB(73.48%)  wps: 3,449  mfu: 20.20%
2024-07-22 20:00:40,558 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,564 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,578 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,580 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,609 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,611 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,611 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,631 - root - INFO - Dumping traces at step 100
2024-07-22 20:00:40,972 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:00:40,974 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:00:40,982 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:00:40,986 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:00:41,019 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:00:41,021 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:00:41,024 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:00:41,039 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,981  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,981  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,981  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,981  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,982  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,982  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,981  mfu: 17.46%
2024-07-22 20:01:04,142 - root - INFO - step: 110  loss:  6.5732  memory: 58.13GiB(73.48%)  wps: 2,983  mfu: 17.47%
2024-07-22 20:01:27,293 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:27,294 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:27,293 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:27,293 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:27,294 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:27,294 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:27,294 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:27,294 - root - INFO - step: 120  loss:  6.5027  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.74%
2024-07-22 20:01:50,434 - root - INFO - step: 130  loss:  6.4431  memory: 58.13GiB(73.48%)  wps: 3,545  mfu: 20.76%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,545  mfu: 20.76%
2024-07-22 20:02:13,576 - root - INFO - step: 140  loss:  6.3903  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.65%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.65%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.65%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.65%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.66%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.65%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,527  mfu: 20.66%
2024-07-22 20:02:36,823 - root - INFO - step: 150  loss:  6.2810  memory: 58.13GiB(73.48%)  wps: 3,526  mfu: 20.65%
2024-07-22 20:02:59,975 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:02:59,975 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:02:59,975 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:02:59,975 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:02:59,976 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:59,976 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:02:59,976 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:02:59,976 - root - INFO - step: 160  loss:  6.3275  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:03:23,139 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:03:23,140 - root - INFO - step: 170  loss:  6.2021  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,454  mfu: 20.23%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,448  mfu: 20.19%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,451  mfu: 20.21%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,448  mfu: 20.19%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,450  mfu: 20.20%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,451  mfu: 20.21%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,450  mfu: 20.20%
2024-07-22 20:03:46,934 - root - INFO - step: 180  loss:  6.1947  memory: 58.13GiB(73.48%)  wps: 3,449  mfu: 20.19%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:04:10,086 - root - INFO - step: 190  loss:  6.1510  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:04:33,249 - root - INFO - step: 200  loss:  6.2636  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:04:37,502 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,502 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,529 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,560 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,578 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,581 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,596 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,668 - root - INFO - Dumping traces at step 200
2024-07-22 20:04:37,894 - root - INFO - Finished dumping traces in 0.39 seconds
2024-07-22 20:04:37,896 - root - INFO - Finished dumping traces in 0.39 seconds
2024-07-22 20:04:37,924 - root - INFO - Finished dumping traces in 0.39 seconds
2024-07-22 20:04:37,971 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:04:37,985 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:04:37,987 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:04:38,000 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:04:38,065 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,938  mfu: 17.21%
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,938  mfu: 17.21%
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,939  mfu: 17.21%
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,939  mfu: 17.21%
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,939  mfu: 17.21%
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,939  mfu: 17.21%
2024-07-22 20:05:01,143 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,941  mfu: 17.22%
2024-07-22 20:05:01,144 - root - INFO - step: 210  loss:  6.1540  memory: 58.13GiB(73.48%)  wps: 2,938  mfu: 17.21%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,545  mfu: 20.76%
2024-07-22 20:05:24,285 - root - INFO - step: 220  loss:  6.0359  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:05:47,440 - root - INFO - step: 230  loss:  5.9928  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:06:10,627 - root - INFO - step: 240  loss:  5.9530  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:06:33,917 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,520  mfu: 20.61%
2024-07-22 20:06:33,917 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,521  mfu: 20.62%
2024-07-22 20:06:33,917 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,522  mfu: 20.62%
2024-07-22 20:06:33,917 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,520  mfu: 20.62%
2024-07-22 20:06:33,917 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,521  mfu: 20.62%
2024-07-22 20:06:33,917 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,520  mfu: 20.62%
2024-07-22 20:06:33,918 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,521  mfu: 20.62%
2024-07-22 20:06:33,919 - root - INFO - step: 250  loss:  5.8827  memory: 58.13GiB(73.48%)  wps: 3,523  mfu: 20.63%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,395  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,395  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,394  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,395  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,396  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,395  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,396  mfu: 19.88%
2024-07-22 20:06:58,072 - root - INFO - step: 260  loss:  5.9025  memory: 58.13GiB(73.48%)  wps: 3,398  mfu: 19.90%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:07:21,216 - root - INFO - step: 270  loss:  5.7826  memory: 58.13GiB(73.48%)  wps: 3,544  mfu: 20.75%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:07:44,374 - root - INFO - step: 280  loss:  5.7318  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:08:07,536 - root - INFO - step: 290  loss:  5.7443  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.66%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.67%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.67%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.67%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,528  mfu: 20.66%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.67%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,528  mfu: 20.66%
2024-07-22 20:08:30,775 - root - INFO - step: 300  loss:  5.6578  memory: 58.13GiB(73.48%)  wps: 3,530  mfu: 20.67%
2024-07-22 20:08:35,154 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,169 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,233 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,248 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,260 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,287 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,295 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,343 - root - INFO - Dumping traces at step 300
2024-07-22 20:08:35,563 - root - INFO - Finished dumping traces in 0.39 seconds
2024-07-22 20:08:35,568 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:08:35,643 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:08:35,665 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:08:35,669 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:08:35,686 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:08:35,699 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:08:35,748 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.10%
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.11%
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.10%
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.11%
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.10%
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.10%
2024-07-22 20:08:58,841 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.10%
2024-07-22 20:08:58,842 - root - INFO - step: 310  loss:  5.6043  memory: 58.13GiB(73.48%)  wps: 2,921  mfu: 17.10%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:09:22,015 - root - INFO - step: 320  loss:  5.5888  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:09:45,171 - root - INFO - step: 330  loss:  5.5921  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,543  mfu: 20.75%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:10:08,328 - root - INFO - step: 340  loss:  5.5557  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:32,154 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,441  mfu: 20.15%
2024-07-22 20:10:32,154 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,443  mfu: 20.16%
2024-07-22 20:10:32,154 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,441  mfu: 20.15%
2024-07-22 20:10:32,155 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,441  mfu: 20.15%
2024-07-22 20:10:32,155 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,441  mfu: 20.15%
2024-07-22 20:10:32,155 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,444  mfu: 20.17%
2024-07-22 20:10:32,155 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,441  mfu: 20.15%
2024-07-22 20:10:32,155 - root - INFO - step: 350  loss:  5.5808  memory: 58.13GiB(73.48%)  wps: 3,442  mfu: 20.16%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:10:55,314 - root - INFO - step: 360  loss:  5.4967  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:11:18,490 - root - INFO - step: 370  loss:  5.5482  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:11:41,685 - root - INFO - step: 380  loss:  5.4796  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,463  mfu: 20.28%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,464  mfu: 20.29%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,464  mfu: 20.28%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,464  mfu: 20.28%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,465  mfu: 20.29%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,463  mfu: 20.28%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,463  mfu: 20.28%
2024-07-22 20:12:05,363 - root - INFO - step: 390  loss:  5.4278  memory: 58.13GiB(73.48%)  wps: 3,464  mfu: 20.28%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,531  mfu: 20.68%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,531  mfu: 20.68%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,531  mfu: 20.68%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,531  mfu: 20.68%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,532  mfu: 20.69%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,532  mfu: 20.68%
2024-07-22 20:12:28,582 - root - INFO - step: 400  loss:  5.4108  memory: 58.13GiB(73.48%)  wps: 3,532  mfu: 20.68%
2024-07-22 20:12:33,028 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,052 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,074 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,086 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,124 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,130 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,144 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,220 - root - INFO - Dumping traces at step 400
2024-07-22 20:12:33,443 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:12:33,450 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:12:33,483 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:12:33,491 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:12:33,527 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:12:33,544 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:12:33,553 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:12:33,625 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.06%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.06%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.06%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,915  mfu: 17.07%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.06%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.07%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.06%
2024-07-22 20:12:56,717 - root - INFO - step: 410  loss:  5.3499  memory: 58.13GiB(73.48%)  wps: 2,914  mfu: 17.06%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.74%
2024-07-22 20:13:19,883 - root - INFO - step: 420  loss:  5.3547  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:13:43,076 - root - INFO - step: 430  loss:  5.3629  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:14:06,269 - root - INFO - step: 440  loss:  5.2864  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,503  mfu: 20.51%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,505  mfu: 20.53%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,503  mfu: 20.51%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,504  mfu: 20.52%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,505  mfu: 20.52%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,504  mfu: 20.52%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,503  mfu: 20.51%
2024-07-22 20:14:29,676 - root - INFO - step: 450  loss:  5.3279  memory: 58.13GiB(73.48%)  wps: 3,505  mfu: 20.52%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:14:52,886 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:14:52,887 - root - INFO - step: 460  loss:  5.2183  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:15:16,097 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:15:16,097 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:15:16,097 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:15:16,097 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:15:16,097 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:15:16,097 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:15:16,098 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:15:16,098 - root - INFO - step: 470  loss:  5.2166  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:15:39,279 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:15:39,279 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:15:39,279 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:15:39,279 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:15:39,279 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:15:39,279 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:15:39,280 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:15:39,280 - root - INFO - step: 480  loss:  5.2315  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,540  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,542  mfu: 20.74%
2024-07-22 20:16:02,446 - root - INFO - step: 490  loss:  5.1918  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:16:25,657 - root - INFO - step: 500  loss:  5.1624  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:16:30,197 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,249 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,266 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,290 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,303 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,304 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,353 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,482 - root - INFO - Dumping traces at step 500
2024-07-22 20:16:30,607 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:16:30,671 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:16:30,695 - root - INFO - Finished dumping traces in 0.43 seconds
2024-07-22 20:16:30,708 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:16:30,720 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:16:30,733 - root - INFO - Finished dumping traces in 0.43 seconds
2024-07-22 20:16:30,753 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:16:30,893 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,891  mfu: 16.93%
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,891  mfu: 16.93%
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,892  mfu: 16.93%
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,892  mfu: 16.93%
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,891  mfu: 16.93%
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,891  mfu: 16.93%
2024-07-22 20:16:54,013 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,891  mfu: 16.93%
2024-07-22 20:16:54,014 - root - INFO - step: 510  loss:  5.1135  memory: 58.13GiB(73.48%)  wps: 2,892  mfu: 16.94%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,434  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,435  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,435  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,434  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,434  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,434  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,434  mfu: 20.11%
2024-07-22 20:17:17,896 - root - INFO - step: 520  loss:  5.1375  memory: 58.13GiB(73.48%)  wps: 3,437  mfu: 20.12%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,532  mfu: 20.68%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:17:41,109 - root - INFO - step: 530  loss:  5.1043  memory: 58.13GiB(73.48%)  wps: 3,532  mfu: 20.68%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.70%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.70%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:18:04,309 - root - INFO - step: 540  loss:  5.1101  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.72%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,507  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,508  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,507  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,507  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,507  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,508  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,508  mfu: 20.54%
2024-07-22 20:18:27,691 - root - INFO - step: 550  loss:  5.1142  memory: 58.13GiB(73.48%)  wps: 3,508  mfu: 20.54%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.72%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,541  mfu: 20.73%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,539  mfu: 20.73%
2024-07-22 20:18:50,867 - root - INFO - step: 560  loss:  5.0853  memory: 58.13GiB(73.48%)  wps: 3,538  mfu: 20.72%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.70%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:19:14,069 - root - INFO - step: 570  loss:  5.0563  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.70%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,536  mfu: 20.71%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:19:37,263 - root - INFO - step: 580  loss:  5.0387  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,533  mfu: 20.69%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,537  mfu: 20.71%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,534  mfu: 20.69%
2024-07-22 20:20:00,468 - root - INFO - step: 590  loss:  5.0430  memory: 58.13GiB(73.48%)  wps: 3,535  mfu: 20.70%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,528  mfu: 20.66%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,530  mfu: 20.67%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,528  mfu: 20.66%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,528  mfu: 20.66%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.66%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,529  mfu: 20.67%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,530  mfu: 20.67%
2024-07-22 20:20:23,709 - root - INFO - step: 600  loss:  4.9773  memory: 58.13GiB(73.48%)  wps: 3,528  mfu: 20.66%
2024-07-22 20:20:28,488 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,544 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,545 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,575 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,608 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,621 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,649 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,692 - root - INFO - Dumping traces at step 600
2024-07-22 20:20:28,939 - root - INFO - Finished dumping traces in 0.45 seconds
2024-07-22 20:20:28,951 - root - INFO - Finished dumping traces in 0.40 seconds
2024-07-22 20:20:28,956 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:20:29,000 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:20:29,025 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:20:29,031 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:20:29,063 - root - INFO - Finished dumping traces in 0.41 seconds
2024-07-22 20:20:29,111 - root - INFO - Finished dumping traces in 0.42 seconds
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,875  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,876  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,876  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,876  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,875  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,876  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,875  mfu: 16.84%
2024-07-22 20:20:52,221 - root - INFO - step: 610  loss:  5.0107  memory: 58.13GiB(73.48%)  wps: 2,876  mfu: 16.84%

------------------------------------------------------------
Sender: LSF System <lsfadmin@p5-r01-n2>
Subject: Job 107729: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 19:55:54 2024
Job was executed on host(s) <p5-r01-n2>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 19:55:55 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 19:55:55 2024
Terminated at Mon Jul 22 20:21:19 2024
Results reported at Mon Jul 22 20:21:19 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   11794.00 sec.
    Max Memory :                                 26870 MB
    Average Memory :                             20520.49 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              14
    Max Threads :                                323
    Run time :                                   1512 sec.
    Turnaround time :                            1525 sec.

The output (if any) is above this job summary.

W0722 20:22:17.990000 22833604024128 torch/distributed/run.py:793] 
W0722 20:22:17.990000 22833604024128 torch/distributed/run.py:793] *****************************************
W0722 20:22:17.990000 22833604024128 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:22:17.990000 22833604024128 torch/distributed/run.py:793] *****************************************
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:23,347 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 20:22:32,385 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:32,395 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:32,425 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:32,641 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:32,641 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:33,463 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:33,466 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:33,474 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:33,477 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:33,479 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:33,492 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:33,668 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:33,669 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:33,676 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:33,676 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:34,024 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:34,027 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:34,029 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:34,129 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:34,132 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:34,134 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:34,149 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:34,155 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:34,170 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:34,174 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:34,176 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:34,190 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:34,201 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:22:34,204 - root - INFO - Building 1-D device mesh with ['dp'], [8]
2024-07-22 20:22:34,214 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:34,214 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:34,225 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:22:34,318 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:34,319 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:34,360 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:34,360 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:34,378 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:34,378 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:34,410 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:22:34,411 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 20:22:40,025 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:40,200 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:40,201 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:22:40,201 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:40,275 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:40,637 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:40,808 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:40,809 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:22:40,809 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:40,875 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:41,390 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:41,563 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:41,564 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:22:41,565 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:41,631 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:41,638 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:41,773 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:41,813 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:41,814 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:22:41,815 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:41,858 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:41,881 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:41,943 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:41,943 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:22:41,944 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:41,971 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:42,010 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:42,030 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:42,030 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:22:42,031 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:42,096 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:42,141 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:42,142 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:22:42,143 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:42,213 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:22:42,213 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:42,385 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 20:22:42,385 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:22:42,386 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 20:22:42,452 - root - INFO - Applied FSDP to the model
2024-07-22 20:22:55,874 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,875 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,875 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,875 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,875 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,876 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,876 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,876 - root - INFO - GPU memory usage for model: 3.78GiB(4.78%)
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,878 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2022
2024-07-22 20:22:55,879 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,879 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,879 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,879 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,879 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,879 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,880 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,880 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:22:55,889 - root - INFO - Training starts at step 1
2024-07-22 20:22:55,889 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,233 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 791  mfu: 4.63%
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - step:  1  loss: 12.2625  memory: 75.22GiB(95.08%)  wps: 792  mfu: 4.64%
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:06,234 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,275 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,275 - root - WARNING - 15 CUDA memory allocation retries.
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,275 - root - WARNING - 14 CUDA memory allocation retries.
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.79GiB(95.81%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:23:58,276 - root - INFO - step: 10  loss: 10.7600  memory: 75.58GiB(95.55%)  wps: 1,417  mfu: 8.30%
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - WARNING - 32 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.79GiB(95.81%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:24:57,795 - root - WARNING - 29 CUDA memory allocation retries.
2024-07-22 20:24:57,795 - root - INFO - step: 20  loss:  9.1190  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:25:56,514 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,513 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - WARNING - 44 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - WARNING - 48 CUDA memory allocation retries.
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.18%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.79GiB(95.81%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:25:56,514 - root - INFO - step: 30  loss:  8.1312  memory: 75.58GiB(95.55%)  wps: 1,396  mfu: 8.17%
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - WARNING - 65 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,113 - root - WARNING - 59 CUDA memory allocation retries.
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.79GiB(95.81%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,113 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:26:58,114 - root - INFO - step: 40  loss:  7.5005  memory: 75.58GiB(95.55%)  wps: 1,331  mfu: 7.79%
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,065 - root - WARNING - 74 CUDA memory allocation retries.
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,065 - root - WARNING - 82 CUDA memory allocation retries.
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.79GiB(95.81%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:27:57,066 - root - INFO - step: 50  loss:  7.3047  memory: 75.58GiB(95.55%)  wps: 1,390  mfu: 8.14%
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.01%
2024-07-22 20:28:56,950 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.01%
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.01%
2024-07-22 20:28:56,950 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.01%
2024-07-22 20:28:56,950 - root - WARNING - 98 CUDA memory allocation retries.
2024-07-22 20:28:56,950 - root - WARNING - 89 CUDA memory allocation retries.
2024-07-22 20:28:56,951 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.01%
2024-07-22 20:28:56,951 - root - INFO - step: 60  loss:  7.1084  memory: 75.79GiB(95.81%)  wps: 1,369  mfu: 8.02%
2024-07-22 20:28:56,951 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.01%
2024-07-22 20:28:56,951 - root - INFO - step: 60  loss:  7.1084  memory: 75.58GiB(95.55%)  wps: 1,369  mfu: 8.02%
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - WARNING - 115 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - WARNING - 104 CUDA memory allocation retries.
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.79GiB(95.81%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:29:57,654 - root - INFO - step: 70  loss:  7.0102  memory: 75.58GiB(95.55%)  wps: 1,350  mfu: 7.91%
2024-07-22 20:30:57,584 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,584 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,584 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,584 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,584 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,584 - root - WARNING - 132 CUDA memory allocation retries.
2024-07-22 20:30:57,584 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.79GiB(95.81%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - WARNING - 119 CUDA memory allocation retries.
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:30:57,585 - root - INFO - step: 80  loss:  6.8598  memory: 75.58GiB(95.55%)  wps: 1,368  mfu: 8.01%
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,111 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,111 - root - WARNING - 134 CUDA memory allocation retries.
2024-07-22 20:31:57,111 - root - WARNING - 148 CUDA memory allocation retries.
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.79GiB(95.81%)  wps: 1,377  mfu: 8.06%
2024-07-22 20:31:57,112 - root - INFO - step: 90  loss:  6.6752  memory: 75.58GiB(95.55%)  wps: 1,377  mfu: 8.07%
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,504 - root - WARNING - 165 CUDA memory allocation retries.
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,504 - root - WARNING - 149 CUDA memory allocation retries.
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.79GiB(95.81%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%
2024-07-22 20:32:59,505 - root - INFO - step: 100  loss:  6.6278  memory: 75.58GiB(95.55%)  wps: 1,314  mfu: 7.69%

------------------------------------------------------------
Sender: LSF System <lsfadmin@p4-r24-n4>
Subject: Job 107737: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 20:22:11 2024
Job was executed on host(s) <p4-r24-n4>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 20:22:11 2024
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 20:22:11 2024
Terminated at Mon Jul 22 20:33:13 2024
Results reported at Mon Jul 22 20:33:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   3980.00 sec.
    Max Memory :                                 20532 MB
    Average Memory :                             12801.72 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              14
    Max Threads :                                307
    Run time :                                   661 sec.
    Turnaround time :                            662 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 20:34:36.089000 23440964409152 torch/distributed/run.py:793] 
W0722 20:34:36.089000 23440964409152 torch/distributed/run.py:793] *****************************************
W0722 20:34:36.089000 23440964409152 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:34:36.089000 23440964409152 torch/distributed/run.py:793] *****************************************
W0722 20:34:36.397000 22838864103232 torch/distributed/run.py:793] 
W0722 20:34:36.397000 22838864103232 torch/distributed/run.py:793] *****************************************
W0722 20:34:36.397000 22838864103232 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:34:36.397000 22838864103232 torch/distributed/run.py:793] *****************************************
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,934 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:39,954 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:41,628 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:34:48,997 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:49,000 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:49,033 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:49,215 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:49,215 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:49,333 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:49,421 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:49,424 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:49,439 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:49,463 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:49,465 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:49,482 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:49,621 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:49,622 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:49,653 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:49,666 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:49,666 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:49,688 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:49,691 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:49,700 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:49,706 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:49,745 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:49,745 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:34:49,892 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:49,892 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:49,924 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:49,936 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:49,938 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:49,954 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:49,959 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:49,977 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:49,991 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:49,994 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:50,005 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:50,008 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:50,009 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:50,011 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:50,013 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:50,014 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:50,031 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:50,073 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,074 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:34:50,114 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,115 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:34:50,166 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:50,166 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:50,201 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:50,201 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:50,209 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:50,209 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:50,209 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:50,217 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:50,217 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:50,234 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:50,249 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:50,251 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,252 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,258 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:50,290 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,292 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,339 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,340 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:34:50,516 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,517 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,598 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:50,633 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,637 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:34:50,653 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,654 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:34:50,666 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,667 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:34:50,670 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:50,670 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:50,671 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:34:50,717 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:50,751 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:50,817 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,819 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,823 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:50,831 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,833 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,843 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,845 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,847 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:50,849 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:50,868 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:50,935 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:51,089 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:51,250 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:51,261 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:51,280 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:51,286 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:51,402 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:51,413 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:51,430 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:51,438 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:51,479 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:51,483 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:51,520 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:51,632 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:51,645 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:51,667 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:51,685 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:51,697 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:51,717 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:51,740 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:51,740 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:51,817 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:51,864 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:51,864 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:51,896 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:51,909 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:51,910 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:51,942 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:51,989 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:51,995 - root - INFO - Training starts at step 1
2024-07-22 20:34:51,995 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,011 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,016 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,016 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,225 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:52,226 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:34:52,269 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,274 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,274 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,285 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,290 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,290 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,307 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:52,308 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:34:52,352 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:52,353 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:34:52,400 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:52,402 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:52,426 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:52,428 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:52,446 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:52,478 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:52,480 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:52,524 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:52,524 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:52,526 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:52,534 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:52,539 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:52,541 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:52,547 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:52,552 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:52,560 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:52,562 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:52,569 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:52,614 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:34:52,617 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [2, 8]
2024-07-22 20:34:52,633 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:34:52,641 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:52,641 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:52,672 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:52,736 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,741 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,741 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,742 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,747 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,747 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,758 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:52,758 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:52,765 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:52,765 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:52,764 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,766 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:52,769 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,770 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,772 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:52,773 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:52,773 - root - INFO - Training starts at step 1
2024-07-22 20:34:52,773 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:52,796 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:52,797 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:52,805 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:52,830 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:34:52,830 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:34:52,861 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:34:52,980 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:52,980 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:52,980 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:53,083 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:53,083 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:34:53,131 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:53,131 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:53,132 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:53,210 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:53,212 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:34:53,213 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:53,214 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:34:53,214 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:53,215 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:34:53,254 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:53,255 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:53,267 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:34:53,267 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:34:53,385 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:53,387 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:53,387 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:53,388 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:53,389 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:53,389 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:53,436 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:34:53,438 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:34:53,671 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:53,812 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:53,812 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:53,820 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:53,827 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:53,855 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:34:53,962 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:53,962 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:53,975 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:54,002 - root - INFO - Applied FSDP to the model
2024-07-22 20:34:54,738 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:54,747 - root - INFO - Training starts at step 1
2024-07-22 20:34:54,747 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:54,805 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:54,810 - root - INFO - Training starts at step 1
2024-07-22 20:34:54,810 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:55,122 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:55,126 - root - INFO - Training starts at step 1
2024-07-22 20:34:55,126 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:55,291 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:55,296 - root - INFO - Training starts at step 1
2024-07-22 20:34:55,296 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:55,362 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:55,368 - root - INFO - Training starts at step 1
2024-07-22 20:34:55,368 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:55,483 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:55,488 - root - INFO - Training starts at step 1
2024-07-22 20:34:55,488 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:55,779 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:55,784 - root - INFO - Training starts at step 1
2024-07-22 20:34:55,784 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:34:56,200 - root - INFO - GPU memory usage for model: 16.47GiB(20.82%)
2024-07-22 20:34:56,206 - root - INFO - Training starts at step 1
2024-07-22 20:34:56,206 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank13]: Traceback (most recent call last):
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank13]:     main(config)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank13]:     return f(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank13]:     optimizers.step()
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank13]:     optimizer.step()
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank13]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank13]:     out = func(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank13]:     ret = func(self, *args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank13]:     adamw(
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank13]:     return func(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank13]:     func(
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank13]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank13]:     return disable_fn(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank13]:     return fn(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank13]:     return DTensor._op_dispatcher.dispatch(
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank13]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank13]:     return self_._op(*args, **kwargs)
[rank13]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 5 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank15]:     main(config)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank15]:     return f(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank15]:     optimizers.step()
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank15]:     optimizer.step()
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank15]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank15]:     out = func(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank15]:     ret = func(self, *args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank15]:     adamw(
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank15]:     return func(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank15]:     func(
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank15]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank15]:     return disable_fn(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank15]:     return fn(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank15]:     return DTensor._op_dispatcher.dispatch(
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank15]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank15]:     return self_._op(*args, **kwargs)
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 7 has a total capacity of 79.11 GiB of which 38.94 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 75.47 GiB is allocated by PyTorch, and 172.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank14]: Traceback (most recent call last):
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank14]:     main(config)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank14]:     return f(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank14]:     optimizers.step()
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank14]:     optimizer.step()
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank14]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank14]:     out = func(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank14]:     ret = func(self, *args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank14]:     adamw(
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank14]:     return func(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank14]:     func(
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank14]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank14]:     return disable_fn(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank14]:     return fn(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank14]:     return DTensor._op_dispatcher.dispatch(
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank14]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank14]:     return self_._op(*args, **kwargs)
[rank14]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank9]: Traceback (most recent call last):
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank9]:     main(config)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank9]:     return f(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank9]:     optimizers.step()
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank9]:     optimizer.step()
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank9]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank9]:     out = func(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank9]:     ret = func(self, *args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank9]:     adamw(
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank9]:     func(
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank9]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank9]:     return disable_fn(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank9]:     return fn(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank9]:     return DTensor._op_dispatcher.dispatch(
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank9]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank9]:     return self_._op(*args, **kwargs)
[rank9]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 1 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank8]: Traceback (most recent call last):
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank8]:     main(config)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank8]:     return f(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank8]:     optimizers.step()
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank8]:     optimizer.step()
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank8]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank8]:     out = func(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank8]:     ret = func(self, *args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank8]:     adamw(
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank8]:     func(
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank8]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank8]:     return disable_fn(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank8]:     return fn(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank8]:     return DTensor._op_dispatcher.dispatch(
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank8]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank8]:     return self_._op(*args, **kwargs)
[rank8]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 50.94 MiB is free. Including non-PyTorch memory, this process has 79.04 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank10]: Traceback (most recent call last):
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank10]:     main(config)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank10]:     return f(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank10]:     optimizers.step()
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank10]:     optimizer.step()
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank10]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank10]:     out = func(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank10]:     ret = func(self, *args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank10]:     adamw(
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank10]:     func(
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank10]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank10]:     return disable_fn(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank10]:     return fn(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank10]:     return DTensor._op_dispatcher.dispatch(
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank10]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank10]:     return self_._op(*args, **kwargs)
[rank10]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank12]: Traceback (most recent call last):
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank12]:     main(config)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank12]:     return f(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank12]:     optimizers.step()
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank12]:     optimizer.step()
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank12]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank12]:     out = func(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank12]:     ret = func(self, *args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank12]:     adamw(
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank12]:     return func(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank12]:     func(
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank12]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank12]:     return disable_fn(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank12]:     return fn(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank12]:     return DTensor._op_dispatcher.dispatch(
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank12]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank12]:     return self_._op(*args, **kwargs)
[rank12]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank4]:     optimizers.step()
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank4]:     optimizer.step()
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank4]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank4]:     out = func(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank4]:     ret = func(self, *args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank4]:     adamw(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank4]:     return func(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank4]:     func(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank4]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank4]:     return DTensor._op_dispatcher.dispatch(
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank4]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank4]:     return self_._op(*args, **kwargs)
[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank1]:     optimizers.step()
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank1]:     optimizer.step()
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank1]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank1]:     adamw(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank1]:     func(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank1]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank1]:     return DTensor._op_dispatcher.dispatch(
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank1]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank1]:     return self_._op(*args, **kwargs)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 1 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank11]: Traceback (most recent call last):
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank11]:     main(config)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank11]:     return f(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank11]:     optimizers.step()
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank11]:     optimizer.step()
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank11]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank11]:     out = func(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank11]:     ret = func(self, *args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank11]:     adamw(
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank11]:     func(
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank11]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank11]:     return disable_fn(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank11]:     return fn(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank11]:     return DTensor._op_dispatcher.dispatch(
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank11]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank11]:     return self_._op(*args, **kwargs)
[rank11]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank3]:     optimizers.step()
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank3]:     optimizer.step()
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank3]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank3]:     ret = func(self, *args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank3]:     adamw(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank3]:     func(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank3]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank3]:     return DTensor._op_dispatcher.dispatch(
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank3]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank3]:     return self_._op(*args, **kwargs)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank5]:     optimizers.step()
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank5]:     optimizer.step()
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank5]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank5]:     out = func(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank5]:     ret = func(self, *args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank5]:     adamw(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank5]:     return func(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank5]:     func(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank5]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank5]:     return DTensor._op_dispatcher.dispatch(
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank5]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank5]:     return self_._op(*args, **kwargs)
[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 5 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank7]:     optimizers.step()
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank7]:     optimizer.step()
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank7]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank7]:     out = func(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank7]:     ret = func(self, *args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank7]:     adamw(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank7]:     return func(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank7]:     func(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank7]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank7]:     return DTensor._op_dispatcher.dispatch(
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank7]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank7]:     return self_._op(*args, **kwargs)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 7 has a total capacity of 79.11 GiB of which 38.94 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 75.47 GiB is allocated by PyTorch, and 172.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank6]:     optimizers.step()
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank6]:     optimizer.step()
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank6]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank6]:     out = func(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank6]:     ret = func(self, *args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank6]:     adamw(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank6]:     func(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank6]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank6]:     return DTensor._op_dispatcher.dispatch(
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank6]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank6]:     return self_._op(*args, **kwargs)
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank2]:     optimizers.step()
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank2]:     optimizer.step()
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank2]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank2]:     adamw(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank2]:     func(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank2]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank2]:     return DTensor._op_dispatcher.dispatch(
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank2]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank2]:     return self_._op(*args, **kwargs)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
[rank0]:     optimizers.step()
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
[rank0]:     optimizer.step()
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
[rank0]:     adamw(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
[rank0]:     func(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
[rank0]:     exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
[rank0]:     return DTensor._op_dispatcher.dispatch(
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
[rank0]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank0]:     return self_._op(*args, **kwargs)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 50.94 MiB is free. Including non-PyTorch memory, this process has 79.04 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank8]:[W722 20:37:59.398160056 ProcessGroupNCCL.cpp:1172] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
[rank0]:[W722 20:38:00.089826962 ProcessGroupNCCL.cpp:1172] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0722 20:38:06.181000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424393 closing signal SIGTERM
W0722 20:38:06.182000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424394 closing signal SIGTERM
W0722 20:38:06.182000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424395 closing signal SIGTERM
W0722 20:38:06.182000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424396 closing signal SIGTERM
W0722 20:38:06.183000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424397 closing signal SIGTERM
W0722 20:38:06.185000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424398 closing signal SIGTERM
W0722 20:38:06.186000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 3424400 closing signal SIGTERM
W0722 20:38:07.382000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273055 closing signal SIGTERM
W0722 20:38:07.382000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273056 closing signal SIGTERM
W0722 20:38:07.382000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273057 closing signal SIGTERM
W0722 20:38:07.382000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273058 closing signal SIGTERM
W0722 20:38:07.382000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273060 closing signal SIGTERM
W0722 20:38:07.383000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273061 closing signal SIGTERM
W0722 20:38:07.383000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 2273062 closing signal SIGTERM
E0722 20:38:09.167000 23440964409152 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 6 (pid: 3424399) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 20:38:09.175000 23440964409152 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_sidhsm0v/none_ypdq35gv/attempt_0/6/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_20:37:53
  host      : p5-r19-n2.bluevela.rmf.ibm.com
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 3424399)
  error_file: /tmp/torchelastic_sidhsm0v/none_ypdq35gv/attempt_0/6/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
      optimizers.step()
    File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
      optimizer.step()
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
      return func.__get__(opt, opt.__class__)(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
      ret = func(self, *args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
      adamw(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
      return func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
      func(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
      exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
      return DTensor._op_dispatcher.dispatch(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
      local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================
E0722 20:38:11.264000 22838864103232 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 4 (pid: 2273059) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 20:38:11.272000 22838864103232 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_6kjsry02/none_upabhqx6/attempt_0/4/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_20:37:53
  host      : p1-r32-n4.bluevela.rmf.ibm.com
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 2273059)
  error_file: /tmp/torchelastic_6kjsry02/none_upabhqx6/attempt_0/4/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 398, in main
      optimizers.step()
    File "/proj/data-eng/lchu/torchtitan/train.py", line 129, in step
      optimizer.step()
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 136, in wrapper
      return func.__get__(opt, opt.__class__)(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 90, in _use_grad
      ret = func(self, *args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 227, in step
      adamw(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/optimizer.py", line 162, in maybe_fallback
      return func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 772, in adamw
      func(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/optim/adamw.py", line 602, in _multi_tensor_adamw
      exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/api.py", line 309, in __torch_dispatch__
      return DTensor._op_dispatcher.dispatch(
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/_tensor/_dispatch.py", line 205, in dispatch
      local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 2.94 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 75.27 GiB is allocated by PyTorch, and 172.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p1-r32-n4>
Subject: Job 107742: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 20:34:29 2024
Job was executed on host(s) <1*p1-r32-n4>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 20:34:29 2024
                            <1*p5-r19-n2>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 20:34:29 2024
Terminated at Mon Jul 22 20:38:12 2024
Results reported at Mon Jul 22 20:38:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   4273.00 sec.
    Max Memory :                                 63212 MB
    Average Memory :                             30129.69 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              552
    Max Threads :                                2177
    Run time :                                   225 sec.
    Turnaround time :                            223 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 20:41:06.818000 23222950987584 torch/distributed/run.py:793] 
W0722 20:41:06.818000 23222950987584 torch/distributed/run.py:793] *****************************************
W0722 20:41:06.818000 23222950987584 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:41:06.818000 23222950987584 torch/distributed/run.py:793] *****************************************
W0722 20:41:06.944000 22778750252864 torch/distributed/run.py:793] 
W0722 20:41:06.944000 22778750252864 torch/distributed/run.py:793] *****************************************
W0722 20:41:06.944000 22778750252864 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:41:06.944000 22778750252864 torch/distributed/run.py:793] *****************************************
W0722 20:41:07.315000 22412692244288 torch/distributed/run.py:793] 
W0722 20:41:07.315000 22412692244288 torch/distributed/run.py:793] *****************************************
W0722 20:41:07.315000 22412692244288 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:41:07.315000 22412692244288 torch/distributed/run.py:793] *****************************************
W0722 20:41:08.424000 22537053230912 torch/distributed/run.py:793] 
W0722 20:41:08.424000 22537053230912 torch/distributed/run.py:793] *****************************************
W0722 20:41:08.424000 22537053230912 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 20:41:08.424000 22537053230912 torch/distributed/run.py:793] *****************************************
2024-07-22 20:41:11,167 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,167 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,167 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,168 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,168 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,168 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,168 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,186 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,186 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,186 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,186 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,187 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,187 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,191 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,191 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,212 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,697 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,697 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,697 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,697 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,697 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,702 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,702 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:11,702 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,329 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:13,338 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 20:41:20,152 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:20,155 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:20,170 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:20,349 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:20,349 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:20,394 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:20,556 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:20,559 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:20,565 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:20,569 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:20,574 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:20,576 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:20,750 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:20,753 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:20,759 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:20,760 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:20,762 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:20,762 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:20,768 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:20,794 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:20,800 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:20,800 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:20,802 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:20,806 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:20,806 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:41:20,809 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:20,812 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:20,818 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:20,836 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:20,954 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:20,954 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:20,979 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:20,980 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:20,999 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,007 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,007 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,019 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,019 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,052 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,080 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:21,083 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:21,098 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:21,102 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:21,105 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:21,110 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,113 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:21,115 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:21,119 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:21,122 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:21,140 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:21,144 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:21,149 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:21,206 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,207 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:41:21,215 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,216 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:41:21,286 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,287 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,306 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,306 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,308 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,308 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,321 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,333 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,334 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,341 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,373 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,380 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,381 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,391 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,392 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,393 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,399 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:21,407 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,407 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:41:21,466 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,467 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:41:21,483 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:21,486 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:21,501 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:21,520 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,520 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:41:21,553 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:21,584 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,586 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,644 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,646 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,683 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:21,683 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:21,695 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,697 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,717 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:21,736 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,736 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:41:21,761 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,762 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:41:21,783 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,784 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:41:21,807 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:21,810 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:21,811 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:41:21,822 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:21,897 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:21,902 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:21,910 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,911 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,920 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:21,941 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,942 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,961 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:21,969 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,970 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:21,975 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:21,991 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:21,992 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:22,053 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,083 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,110 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,110 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,130 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:22,131 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:41:22,146 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,204 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,240 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,253 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,256 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,262 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,274 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,305 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:22,306 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:22,333 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,379 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,381 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,390 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,409 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,413 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,417 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,426 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,433 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,436 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,438 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,441 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,441 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,453 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,453 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,456 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,486 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,490 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,500 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,503 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,507 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,518 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,521 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,525 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,534 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,548 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,551 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,555 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:22,555 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,556 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:41:22,557 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,561 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,566 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,567 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,567 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,570 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,570 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,575 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,580 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,582 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,584 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,586 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,587 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,593 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,601 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,601 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,623 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:22,626 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:22,633 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,634 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,634 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,643 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:22,646 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,646 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,671 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,681 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,699 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,699 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,719 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,719 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,723 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:22,727 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:22,729 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:22,735 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,745 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,745 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,751 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,751 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,756 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,767 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,767 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,781 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,782 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,784 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,787 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,787 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,791 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,800 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,819 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,827 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,840 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:22,841 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:22,877 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:22,880 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:22,904 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:22,905 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:41:23,044 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,044 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:41:23,082 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,083 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,084 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,085 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:41:23,102 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,103 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:41:23,114 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,119 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,119 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,152 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,154 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,155 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:41:23,156 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,160 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,161 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,161 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,165 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,165 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,172 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,173 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:41:23,175 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:23,179 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:23,198 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,198 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:41:23,209 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,210 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:41:23,216 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,217 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,217 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:41:23,219 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,218 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:23,234 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,235 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:41:23,242 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,243 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:41:23,261 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,264 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,282 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,284 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,294 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,295 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:41:23,303 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,331 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,333 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,349 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,351 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,365 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,370 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,370 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,371 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,373 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,388 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,389 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,393 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,395 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,409 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,411 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,416 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,417 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,428 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:23,428 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:23,472 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:23,473 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:23,501 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:23,514 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,650 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,665 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,672 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,678 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,678 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,700 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,716 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,721 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,721 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,727 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,757 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,784 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,786 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,792 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,792 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,802 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,805 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,824 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,833 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,834 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,836 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,837 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,839 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,839 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,842 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,842 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,845 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,851 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,867 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:23,872 - root - INFO - Training starts at step 1
2024-07-22 20:41:23,872 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:23,882 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,910 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,911 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:23,912 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:23,912 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 20:41:23,934 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,955 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,976 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,985 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,990 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:23,998 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:24,061 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:24,089 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:24,091 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:24,334 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:24,339 - root - INFO - Training starts at step 1
2024-07-22 20:41:24,340 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:24,510 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:24,513 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:24,530 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:24,591 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:24,594 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:24,608 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:24,680 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:24,718 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:24,718 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:24,753 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:24,788 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:24,793 - root - INFO - Training starts at step 1
2024-07-22 20:41:24,793 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:24,795 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:24,795 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:24,828 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:24,840 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:25,016 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,021 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,022 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,121 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,126 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,126 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,166 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:25,166 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 20:41:25,230 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,235 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,235 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,239 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:25,239 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 20:41:25,315 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:25,318 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:25,321 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:25,340 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:25,341 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:25,393 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,398 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,398 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,400 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:25,403 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:25,406 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:25,410 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:25,412 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:25,417 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:25,418 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:25,421 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:25,424 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:25,437 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:25,440 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,442 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:25,445 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,445 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,448 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 20:41:25,452 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [4, 8]
2024-07-22 20:41:25,455 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 20:41:25,513 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:25,513 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:25,546 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:25,601 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:25,601 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:25,622 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:25,623 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:25,628 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:25,628 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:25,634 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:25,651 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 20:41:25,652 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 20:41:25,658 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:25,668 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:25,684 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 20:41:25,765 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:25,781 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,786 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,786 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,788 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,793 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,794 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,830 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:25,876 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,881 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,881 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,918 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:25,931 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,936 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,936 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,959 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:25,961 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:25,962 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 20:41:25,965 - root - INFO - Training starts at step 1
2024-07-22 20:41:25,965 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:25,982 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:26,048 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:26,048 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 20:41:26,074 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:26,074 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 20:41:26,083 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:26,085 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 20:41:26,092 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:26,097 - root - INFO - Training starts at step 1
2024-07-22 20:41:26,097 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:26,098 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 20:41:26,098 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 20:41:26,136 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:26,138 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:26,144 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:26,150 - root - INFO - Training starts at step 1
2024-07-22 20:41:26,150 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:26,221 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:26,222 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:26,249 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:26,251 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:26,260 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:26,261 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:26,272 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 20:41:26,273 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 20:41:26,554 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:26,639 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:26,683 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:26,695 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:26,700 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 20:41:26,709 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:26,793 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:26,836 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:26,848 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:26,852 - root - INFO - Applied FSDP to the model
2024-07-22 20:41:27,182 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:27,193 - root - INFO - Training starts at step 1
2024-07-22 20:41:27,193 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:27,355 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:27,360 - root - INFO - Training starts at step 1
2024-07-22 20:41:27,360 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:27,509 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:27,514 - root - INFO - Training starts at step 1
2024-07-22 20:41:27,515 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:28,465 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:28,470 - root - INFO - Training starts at step 1
2024-07-22 20:41:28,470 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:28,522 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:28,527 - root - INFO - Training starts at step 1
2024-07-22 20:41:28,527 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:28,887 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:28,892 - root - INFO - Training starts at step 1
2024-07-22 20:41:28,892 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:28,894 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:28,899 - root - INFO - Training starts at step 1
2024-07-22 20:41:28,899 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 20:41:29,121 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 20:41:29,126 - root - INFO - Training starts at step 1
2024-07-22 20:41:29,126 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-22 20:44:31,770 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,769 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,771 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,770 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,769 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,770 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:31,770 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,770 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,833 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,832 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,832 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,833 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,833 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,833 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,832 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,833 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,837 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,837 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,837 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:31,838 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,837 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,837 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,837 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,838 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,851 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:31,850 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,850 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,851 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,851 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,851 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,851 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,852 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,921 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:31,921 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,921 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:31,921 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,922 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,922 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:31,921 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:31,923 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,056 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:32,055 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:32,056 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:32,057 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,056 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,056 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,058 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:32,059 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,074 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:32,073 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:32,073 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:32,073 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:32,074 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,073 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,074 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,073 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,077 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.27%
2024-07-22 20:44:32,076 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:32,077 - root - INFO - step:  1  loss: 12.2375  memory: 43.93GiB(55.54%)  wps: 5  mfu: 0.26%
2024-07-22 20:44:32,078 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,077 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,076 - root - INFO - step:  1  loss: 12.2375  memory: 43.90GiB(55.50%)  wps: 6  mfu: 0.27%
2024-07-22 20:44:32,077 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:44:32,077 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 315  mfu: 15.32%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 315  mfu: 15.32%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 315  mfu: 15.32%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 315  mfu: 15.32%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 313  mfu: 15.24%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 314  mfu: 15.28%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 316  mfu: 15.39%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 314  mfu: 15.28%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 314  mfu: 15.28%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 314  mfu: 15.28%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 316  mfu: 15.39%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 313  mfu: 15.24%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 314  mfu: 15.27%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 316  mfu: 15.40%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.24GiB(67.30%)  wps: 316  mfu: 15.39%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 313  mfu: 15.24%
2024-07-22 20:45:01,209 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 313  mfu: 15.24%
2024-07-22 20:45:01,210 - root - INFO - step: 10  loss: 11.2132  memory: 53.21GiB(67.27%)  wps: 316  mfu: 15.39%
2024-07-22 20:45:09,860 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,860 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,885 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,908 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,909 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,922 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,925 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,933 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,934 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,958 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,966 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,966 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,969 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,973 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:09,985 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,007 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,010 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,014 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,029 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,044 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,065 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,092 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,096 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,127 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,187 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,218 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,260 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,338 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,400 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,417 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,437 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,598 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:10,607 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:45:10,615 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:10,643 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:10,645 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:10,654 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:10,663 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:10,855 - root - INFO - Finished dumping traces in 0.93 seconds
2024-07-22 20:45:10,856 - root - INFO - Dumping traces at step 10
2024-07-22 20:45:10,936 - root - INFO - Finished dumping traces in 1.00 seconds
2024-07-22 20:45:10,935 - root - INFO - Finished dumping traces in 0.97 seconds
2024-07-22 20:45:10,935 - root - INFO - Finished dumping traces in 0.96 seconds
2024-07-22 20:45:10,935 - root - INFO - Finished dumping traces in 0.92 seconds
2024-07-22 20:45:11,014 - root - INFO - Finished dumping traces in 1.03 seconds
2024-07-22 20:45:11,014 - root - INFO - Finished dumping traces in 0.92 seconds
2024-07-22 20:45:11,014 - root - INFO - Finished dumping traces in 1.00 seconds
2024-07-22 20:45:11,014 - root - INFO - Finished dumping traces in 1.05 seconds
2024-07-22 20:45:11,046 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:45:11,046 - root - INFO - Finished dumping traces in 1.04 seconds
2024-07-22 20:45:11,046 - root - INFO - Finished dumping traces in 1.02 seconds
2024-07-22 20:45:11,054 - root - INFO - Finished dumping traces in 0.99 seconds
2024-07-22 20:45:11,054 - root - INFO - Finished dumping traces in 1.01 seconds
2024-07-22 20:45:11,058 - root - INFO - Finished dumping traces in 0.93 seconds
2024-07-22 20:45:11,058 - root - INFO - Finished dumping traces in 0.96 seconds
2024-07-22 20:45:11,061 - root - INFO - Finished dumping traces in 1.10 seconds
2024-07-22 20:45:11,061 - root - INFO - Finished dumping traces in 1.09 seconds
2024-07-22 20:45:11,079 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:11,141 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:11,165 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:45:11,197 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:45:11,270 - root - INFO - Finished dumping traces in 1.01 seconds
2024-07-22 20:45:11,389 - root - INFO - Finished dumping traces in 1.20 seconds
2024-07-22 20:45:11,807 - root - INFO - Finished dumping traces in 0.95 seconds
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,474 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,472 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.21GiB(67.27%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:29,473 - root - INFO - step: 20  loss:  9.3853  memory: 53.24GiB(67.30%)  wps: 362  mfu: 17.64%
2024-07-22 20:45:38,678 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,742 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,779 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,794 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,812 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,812 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,820 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,867 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,883 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,901 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,937 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,963 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,971 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,977 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:38,977 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,002 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,009 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,016 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,019 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,029 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,073 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,107 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,108 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,117 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,141 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,224 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,237 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,263 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,274 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,299 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,385 - root - INFO - Finished dumping traces in 0.71 seconds
2024-07-22 20:45:39,462 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,476 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:39,533 - root - INFO - Dumping traces at step 20
2024-07-22 20:45:39,534 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:45:39,536 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:39,541 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:39,554 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:39,603 - root - INFO - Finished dumping traces in 0.72 seconds
2024-07-22 20:45:39,623 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:45:39,649 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:45:39,705 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:39,717 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:39,729 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:45:39,779 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:45:39,792 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:45:39,792 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:45:39,809 - root - INFO - Finished dumping traces in 0.99 seconds
2024-07-22 20:45:39,813 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:45:39,813 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:39,824 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:45:39,854 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:45:39,861 - root - INFO - Finished dumping traces in 0.72 seconds
2024-07-22 20:45:39,894 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:45:39,986 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:45:40,018 - root - INFO - Finished dumping traces in 0.99 seconds
2024-07-22 20:45:40,023 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:45:40,066 - root - INFO - Finished dumping traces in 1.13 seconds
2024-07-22 20:45:40,069 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:45:40,069 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:45:40,070 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:45:40,198 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:45:40,218 - root - INFO - Finished dumping traces in 1.11 seconds
2024-07-22 20:45:40,296 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,994 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.24GiB(67.30%)  wps: 359  mfu: 17.48%
2024-07-22 20:45:57,993 - root - INFO - step: 30  loss:  8.7607  memory: 53.21GiB(67.27%)  wps: 359  mfu: 17.48%
2024-07-22 20:46:07,163 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,232 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,266 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,317 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,324 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,331 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,384 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,455 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,458 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,462 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,466 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,465 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,479 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,480 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,518 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,538 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,544 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,558 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,565 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,568 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,593 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,604 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,613 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,614 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,674 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,809 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,846 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,867 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,876 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,876 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,902 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,903 - root - INFO - Dumping traces at step 30
2024-07-22 20:46:07,921 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:07,991 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:08,010 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:08,059 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:08,083 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:08,091 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:08,149 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:08,288 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:46:08,293 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:46:08,294 - root - INFO - Finished dumping traces in 0.84 seconds
2024-07-22 20:46:08,294 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:46:08,299 - root - INFO - Finished dumping traces in 0.84 seconds
2024-07-22 20:46:08,298 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:08,298 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:46:08,302 - root - INFO - Finished dumping traces in 0.84 seconds
2024-07-22 20:46:08,302 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:08,348 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:46:08,350 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:08,369 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:46:08,423 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:08,439 - root - INFO - Finished dumping traces in 0.89 seconds
2024-07-22 20:46:08,521 - root - INFO - Finished dumping traces in 0.95 seconds
2024-07-22 20:46:08,521 - root - INFO - Finished dumping traces in 0.96 seconds
2024-07-22 20:46:08,568 - root - INFO - Finished dumping traces in 0.97 seconds
2024-07-22 20:46:08,590 - root - INFO - Finished dumping traces in 0.99 seconds
2024-07-22 20:46:08,594 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:08,608 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:08,636 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:46:08,656 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:08,660 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:08,670 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:46:09,000 - root - INFO - Finished dumping traces in 1.19 seconds
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,583 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.24GiB(67.30%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:26,582 - root - INFO - step: 40  loss:  8.1304  memory: 53.21GiB(67.27%)  wps: 358  mfu: 17.44%
2024-07-22 20:46:36,193 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,232 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,253 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,266 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,274 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,361 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,380 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,400 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,404 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,420 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,429 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,458 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,500 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,501 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,506 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,516 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,526 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,539 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,610 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,619 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,647 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,651 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,680 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,716 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,723 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,756 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,777 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,785 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,797 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,830 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,890 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,938 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:36,957 - root - INFO - Dumping traces at step 40
2024-07-22 20:46:36,972 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:36,998 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:37,003 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:46:37,009 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:37,105 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:46:37,137 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:37,157 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:37,203 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:37,215 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:46:37,216 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:46:37,221 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:37,246 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:37,262 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:37,273 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:37,284 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:37,295 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:46:37,331 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:46:37,334 - root - INFO - Finished dumping traces in 0.72 seconds
2024-07-22 20:46:37,379 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:46:37,386 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:46:37,426 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:46:37,491 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:46:37,495 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:37,550 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:46:37,554 - root - INFO - Finished dumping traces in 0.90 seconds
2024-07-22 20:46:37,554 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:37,575 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:46:37,576 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:46:37,618 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:46:37,657 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:46:37,684 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,924 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.24GiB(67.30%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:03,923 - root - INFO - step: 50  loss:  8.1738  memory: 53.21GiB(67.27%)  wps: 274  mfu: 13.35%
2024-07-22 20:47:14,294 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,331 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,439 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,541 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,545 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,552 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,607 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,621 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,629 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,629 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,659 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,668 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,674 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,676 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,742 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,745 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,755 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,787 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,799 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,833 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,836 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,840 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,877 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,908 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,949 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:14,950 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,011 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,012 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,016 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,023 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,072 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,105 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:15,173 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,245 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:47:15,250 - root - INFO - Dumping traces at step 50
2024-07-22 20:47:15,335 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,337 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:47:15,344 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:47:15,386 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:15,391 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,419 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:47:15,420 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:47:15,443 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:47:15,454 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,469 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:47:15,483 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:47:15,514 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:15,519 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:15,522 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,579 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:47:15,628 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:47:15,640 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:47:15,640 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:47:15,653 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,664 - root - INFO - Finished dumping traces in 0.88 seconds
2024-07-22 20:47:15,712 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:47:15,729 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:15,760 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:47:15,797 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:47:15,832 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:47:15,847 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:47:15,856 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:47:15,918 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:47:15,999 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,773 - root - INFO - step: 60  loss:  7.7900  memory: 53.21GiB(67.27%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:33,774 - root - INFO - step: 60  loss:  7.7900  memory: 53.24GiB(67.30%)  wps: 343  mfu: 16.70%
2024-07-22 20:47:44,285 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,302 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,326 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,348 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,360 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,359 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,361 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,369 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,396 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,404 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,413 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,435 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,438 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,461 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,461 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,463 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,463 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,544 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,570 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,585 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,586 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,598 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,598 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,602 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,622 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,630 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,632 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,641 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,660 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,687 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:44,744 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:45,025 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:47:45,076 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:45,078 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:47:45,090 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:47:45,094 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:47:45,126 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:45,135 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,140 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,172 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:45,179 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,182 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:45,195 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:47:45,201 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:45,219 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,224 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:45,226 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:45,258 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:47:45,309 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 20:47:45,314 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:45,340 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:47:45,363 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:45,367 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:45,378 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:47:45,382 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,383 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:47:45,391 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 20:47:45,400 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:47:45,408 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,421 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:47:45,475 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:47:45,501 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:47:47,383 - root - INFO - Dumping traces at step 60
2024-07-22 20:47:48,203 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,066 - root - INFO - step: 70  loss:  7.5365  memory: 53.24GiB(67.30%)  wps: 317  mfu: 15.45%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:06,065 - root - INFO - step: 70  loss:  7.5365  memory: 53.21GiB(67.27%)  wps: 317  mfu: 15.44%
2024-07-22 20:48:16,250 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,301 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,344 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,397 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,402 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,417 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,450 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,467 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,494 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,511 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,514 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,517 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,518 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,523 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,559 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,564 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,576 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,579 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,597 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,599 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,610 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,623 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,628 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,631 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,637 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,647 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,650 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,669 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,697 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,725 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:16,794 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:17,020 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:17,103 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,154 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:48:17,200 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,203 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,230 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,234 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:48:17,280 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:48:17,280 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:17,295 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,301 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,307 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:48:17,310 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,335 - root - INFO - Finished dumping traces in 0.84 seconds
2024-07-22 20:48:17,348 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,350 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:48:17,355 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,358 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,361 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,395 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,406 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,418 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:48:17,428 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:48:17,445 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:17,451 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:48:17,458 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:48:17,469 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:48:17,478 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:48:17,522 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:17,525 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:48:17,586 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:48:17,741 - root - INFO - Dumping traces at step 70
2024-07-22 20:48:18,515 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,263 - root - INFO - step: 80  loss:  7.3336  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:36,264 - root - INFO - step: 80  loss:  7.3336  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.51%
2024-07-22 20:48:46,808 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,876 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,932 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,951 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,951 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,952 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,964 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:46,964 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,021 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,056 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,060 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,089 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,108 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,118 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,153 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,186 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,201 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,201 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,214 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,216 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,217 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,220 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,240 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,250 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,259 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,261 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,278 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,289 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,310 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,327 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,405 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,565 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:48:47,625 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:48:47,708 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:47,734 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:47,737 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:47,739 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:47,746 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:47,749 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:47,756 - root - INFO - Dumping traces at step 80
2024-07-22 20:48:47,786 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:48:47,840 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:47,862 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:47,876 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:48:47,876 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:47,879 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:48:47,904 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:48:47,956 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:48:47,963 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 20:48:47,968 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:47,979 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:48:47,991 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:47,997 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:48,021 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:48,060 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:48,063 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:48:48,108 - root - INFO - Finished dumping traces in 0.87 seconds
2024-07-22 20:48:48,113 - root - INFO - Finished dumping traces in 0.90 seconds
2024-07-22 20:48:48,114 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:48:48,113 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:48:48,125 - root - INFO - Finished dumping traces in 0.86 seconds
2024-07-22 20:48:48,162 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:48:48,311 - root - INFO - Finished dumping traces in 1.05 seconds
2024-07-22 20:48:48,557 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,297 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,295 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,295 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,295 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:06,296 - root - INFO - step: 90  loss:  7.1953  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.60%
2024-07-22 20:49:16,622 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,737 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,755 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,759 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,759 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,760 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,809 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,822 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,836 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,859 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,877 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,894 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,914 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,931 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,931 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,945 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,967 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,988 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:16,989 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,016 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,026 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,034 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,056 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,071 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,080 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,080 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,081 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,087 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,112 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,140 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,209 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,397 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:49:17,526 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:17,544 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:17,558 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:17,569 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,570 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,574 - root - INFO - Dumping traces at step 90
2024-07-22 20:49:17,602 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:17,617 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:17,638 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:17,665 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,700 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:17,708 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,708 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:17,711 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:49:17,727 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:17,739 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:17,784 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:49:17,785 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:17,791 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:17,794 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:17,815 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:49:17,821 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:17,832 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,862 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:17,876 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:17,886 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:49:17,892 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,892 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,894 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:17,910 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:49:17,996 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:18,335 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,010 - root - INFO - step: 100  loss:  7.0417  memory: 53.21GiB(67.27%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:40,011 - root - INFO - step: 100  loss:  7.0417  memory: 53.24GiB(67.30%)  wps: 304  mfu: 14.79%
2024-07-22 20:49:50,717 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,761 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,770 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,801 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,802 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,840 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,905 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,947 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,977 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,980 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:50,988 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,001 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,020 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,026 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,038 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,039 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,058 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,063 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,065 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,069 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,088 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,132 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,154 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,169 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,176 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,207 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,230 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,233 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,235 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,249 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,268 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,488 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:49:51,521 - root - INFO - Dumping traces at step 100
2024-07-22 20:49:51,590 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:49:51,595 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,595 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,595 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:49:51,628 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,720 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:49:51,738 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,766 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,790 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,814 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:49:51,813 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:49:51,818 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:51,821 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:51,826 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:49:51,849 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:51,849 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:51,862 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:49:51,886 - root - INFO - Finished dumping traces in 0.91 seconds
2024-07-22 20:49:51,913 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:51,944 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:51,955 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:51,977 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:49:51,979 - root - INFO - Finished dumping traces in 0.95 seconds
2024-07-22 20:49:51,999 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:49:52,010 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:52,025 - root - INFO - Finished dumping traces in 0.89 seconds
2024-07-22 20:49:52,025 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:49:52,049 - root - INFO - Finished dumping traces in 0.98 seconds
2024-07-22 20:49:52,056 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:49:52,175 - root - INFO - Finished dumping traces in 0.91 seconds
2024-07-22 20:49:52,304 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,075 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.59%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.59%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,077 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.21GiB(67.27%)  wps: 341  mfu: 16.58%
2024-07-22 20:50:10,076 - root - INFO - step: 110  loss:  6.9571  memory: 53.24GiB(67.30%)  wps: 341  mfu: 16.59%
2024-07-22 20:50:21,030 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,114 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,140 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,142 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,149 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,151 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,154 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,157 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,220 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,281 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,282 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,283 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,308 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,336 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,336 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,342 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,347 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,358 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,362 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,396 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,402 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,406 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,408 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,411 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,415 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,415 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,447 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,447 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,448 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,487 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,687 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,711 - root - INFO - Dumping traces at step 110
2024-07-22 20:50:21,800 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:21,955 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:21,970 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:50:21,971 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:50:21,970 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:50:21,997 - root - INFO - Finished dumping traces in 0.88 seconds
2024-07-22 20:50:21,998 - root - INFO - Finished dumping traces in 0.84 seconds
2024-07-22 20:50:22,000 - root - INFO - Finished dumping traces in 0.86 seconds
2024-07-22 20:50:22,049 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:22,067 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:50:22,072 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:22,166 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:22,171 - root - INFO - Finished dumping traces in 0.86 seconds
2024-07-22 20:50:22,170 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:22,173 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:22,173 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:22,173 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:22,173 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:22,199 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:22,207 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:22,208 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:22,220 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:22,223 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:22,240 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:22,260 - root - INFO - Finished dumping traces in 0.98 seconds
2024-07-22 20:50:22,262 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:22,263 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:22,280 - root - INFO - Finished dumping traces in 0.87 seconds
2024-07-22 20:50:22,291 - root - INFO - Finished dumping traces in 0.90 seconds
2024-07-22 20:50:22,452 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:22,479 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:22,541 - root - INFO - Finished dumping traces in 1.05 seconds
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,308 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.24GiB(67.30%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:40,307 - root - INFO - step: 120  loss:  6.8769  memory: 53.21GiB(67.27%)  wps: 339  mfu: 16.49%
2024-07-22 20:50:51,040 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,053 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,063 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,096 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,100 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,127 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,146 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,162 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,185 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,248 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,250 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,275 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,286 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,287 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,287 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,307 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,324 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,327 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,339 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,343 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,369 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,374 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,374 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,380 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,402 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,424 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,428 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,429 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,441 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,465 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,470 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,594 - root - INFO - Dumping traces at step 120
2024-07-22 20:50:51,910 - root - INFO - Finished dumping traces in 0.86 seconds
2024-07-22 20:50:51,913 - root - INFO - Finished dumping traces in 0.87 seconds
2024-07-22 20:50:51,914 - root - INFO - Finished dumping traces in 0.85 seconds
2024-07-22 20:50:51,917 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:50:51,918 - root - INFO - Finished dumping traces in 0.82 seconds
2024-07-22 20:50:51,944 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:50:51,973 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:51,980 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:52,039 - root - INFO - Finished dumping traces in 0.91 seconds
2024-07-22 20:50:52,046 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:50:52,053 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:52,055 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:52,081 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:50:52,082 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:52,096 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:52,109 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 20:50:52,130 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 20:50:52,160 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:52,175 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:52,188 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 20:50:52,192 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 20:50:52,192 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 20:50:52,213 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:50:52,241 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:50:52,254 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 20:50:52,295 - root - INFO - Finished dumping traces in 0.92 seconds
2024-07-22 20:50:52,340 - root - INFO - Finished dumping traces in 1.09 seconds
2024-07-22 20:50:52,389 - root - INFO - Finished dumping traces in 1.05 seconds
2024-07-22 20:50:52,392 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 20:50:52,400 - root - INFO - Finished dumping traces in 1.06 seconds
2024-07-22 20:50:52,419 - root - INFO - Finished dumping traces in 0.98 seconds
2024-07-22 20:50:52,505 - root - INFO - Finished dumping traces in 1.13 seconds

------------------------------------------------------------
Sender: LSF System <lsfadmin@p1-r32-n4>
Subject: Job 107744: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 20:41:00 2024
Job was executed on host(s) <1*p1-r32-n4>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 20:41:00 2024
                            <1*p4-r24-n4>
                            <1*p5-r06-n1>
                            <1*p5-r19-n2>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 20:41:00 2024
Terminated at Mon Jul 22 20:51:15 2024
Results reported at Mon Jul 22 20:51:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   27997.00 sec.
    Max Memory :                                 319773 MB
    Average Memory :                             154154.45 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              1100
    Max Threads :                                4182
    Run time :                                   621 sec.
    Turnaround time :                            615 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 21:19:02.717000 23059018012480 torch/distributed/run.py:793] 
W0722 21:19:02.717000 23059018012480 torch/distributed/run.py:793] *****************************************
W0722 21:19:02.717000 23059018012480 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 21:19:02.717000 23059018012480 torch/distributed/run.py:793] *****************************************
W0722 21:19:04.315000 23396235732800 torch/distributed/run.py:793] 
W0722 21:19:04.315000 23396235732800 torch/distributed/run.py:793] *****************************************
W0722 21:19:04.315000 23396235732800 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 21:19:04.315000 23396235732800 torch/distributed/run.py:793] *****************************************
W0722 21:19:04.334000 22738615514944 torch/distributed/run.py:793] 
W0722 21:19:04.334000 22738615514944 torch/distributed/run.py:793] *****************************************
W0722 21:19:04.334000 22738615514944 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 21:19:04.334000 22738615514944 torch/distributed/run.py:793] *****************************************
W0722 21:19:04.510000 23019116988224 torch/distributed/run.py:793] 
W0722 21:19:04.510000 23019116988224 torch/distributed/run.py:793] *****************************************
W0722 21:19:04.510000 23019116988224 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 21:19:04.510000 23019116988224 torch/distributed/run.py:793] *****************************************
2024-07-22 21:19:07,834 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,834 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,834 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,834 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,834 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,834 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,848 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:07,849 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,367 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,373 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,385 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,392 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:09,470 - root - INFO - Starting job: Llama 3 70B training
2024-07-22 21:19:17,083 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,089 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,105 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,296 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:17,296 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:17,356 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:17,648 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,651 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,666 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,693 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,706 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,755 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,769 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:17,770 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 21:19:17,794 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,797 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,797 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,800 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,801 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,803 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,806 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,811 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,815 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,838 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,841 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,846 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,855 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,857 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:17,857 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,859 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:17,859 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:17,860 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:17,861 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,865 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:17,891 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:17,944 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:17,945 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:17,945 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:17,945 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:17,995 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:17,995 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,003 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,004 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,008 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,008 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,031 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,038 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,040 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,040 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,041 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,056 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,056 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,057 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,060 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,061 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,073 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,088 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,098 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,152 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,160 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,191 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,310 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,311 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 21:19:18,408 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,408 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,419 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:18,449 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,450 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 21:19:18,454 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,454 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,455 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 21:19:18,455 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 21:19:18,477 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,478 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 21:19:18,480 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,481 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,488 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,489 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 21:19:18,497 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,497 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 21:19:18,510 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,517 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,518 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 21:19:18,579 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:18,623 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,625 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,655 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,656 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,666 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,666 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,666 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,666 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,667 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,667 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,667 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,668 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,689 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:18,691 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:18,734 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,737 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,742 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,745 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,749 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,749 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,752 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,752 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,752 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,756 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,761 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,762 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,763 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,766 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,779 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,799 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,799 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,802 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,802 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,808 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,815 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,817 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,818 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,821 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,823 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:18,826 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:18,829 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:18,901 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:18,925 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:18,926 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 21:19:18,939 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,939 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,948 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,948 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,953 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,953 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,964 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,964 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,969 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,972 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:18,972 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:18,984 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,989 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:18,997 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,004 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,006 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,006 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,014 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,014 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,017 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,017 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,023 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,025 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,026 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,025 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,027 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,028 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,030 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,032 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,039 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,046 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,047 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,049 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,058 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,060 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,088 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,091 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,093 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,099 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,101 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,101 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,129 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,188 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,191 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,194 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,206 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,219 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,219 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,220 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,220 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,226 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,248 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,249 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,253 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,253 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,255 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,260 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,288 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,296 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,301 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,302 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,305 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,309 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,317 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,383 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,383 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 21:19:19,384 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,384 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,390 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,397 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,397 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 21:19:19,403 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,403 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 21:19:19,409 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,409 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 21:19:19,418 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,418 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 21:19:19,417 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,451 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,452 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 21:19:19,462 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,462 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 21:19:19,467 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,469 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 21:19:19,475 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,475 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 21:19:19,498 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,498 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,505 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,505 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,525 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,529 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,534 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,545 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,564 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,566 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,566 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,572 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,574 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,576 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,577 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,584 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,586 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,624 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,626 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,629 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,633 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,635 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,637 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,643 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,644 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,666 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,667 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 21:19:19,672 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,672 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,672 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 21:19:19,789 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:19,789 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:19,829 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,829 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 21:19:19,833 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:19,841 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,843 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:19,901 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:19,943 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,943 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 21:19:19,955 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,958 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,962 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:19,963 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 21:19:19,970 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:19,973 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:19,973 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,989 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:19,992 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,995 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:19,998 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:19,997 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,001 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,006 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,025 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,029 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,030 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,055 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,055 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,068 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,074 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,112 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,112 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,112 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,114 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,114 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,114 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,131 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,133 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,150 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,158 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,159 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,163 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,168 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:20,168 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:20,180 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:20,180 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:20,202 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:20,210 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:20,213 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,215 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,227 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,233 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,257 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,262 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,262 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,269 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,276 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,283 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,284 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,320 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:20,323 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 21:19:20,395 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,401 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,401 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,418 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,428 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,453 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,459 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,459 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,465 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,470 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,470 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,481 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,488 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,488 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,498 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,499 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,533 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,534 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,536 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,536 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,539 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,539 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,564 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:20,575 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,608 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:20,609 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 21:19:20,612 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,620 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:20,621 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 21:19:20,626 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,626 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,690 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,695 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,695 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,724 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:20,775 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,777 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,791 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:20,793 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:20,989 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,995 - root - INFO - Training starts at step 1
2024-07-22 21:19:20,995 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:20,994 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:20,995 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,006 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,006 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,006 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,006 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,031 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:21,034 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:21,049 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:21,051 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:21,054 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:21,054 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:21,057 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:21,060 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:21,069 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:21,076 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:21,079 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,084 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,084 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,116 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:21,119 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:21,119 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 21:19:21,123 - root - INFO - Building 2-D device mesh with ['dp', 'tp'], [8, 4]
2024-07-22 21:19:21,128 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:21,136 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 21:19:21,175 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,175 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,180 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,180 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,180 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,180 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,189 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,194 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,195 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,202 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:21,203 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,206 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:21,209 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,209 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,215 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,220 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,220 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,228 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,233 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,233 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,236 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:21,243 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:21,243 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:21,256 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:21,256 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:21,266 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:21,266 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:21,275 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:21,286 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:21,299 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:21,324 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:21,325 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:21,334 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 21:19:21,335 - root - INFO - Preparing c4_mini dataset from torchtitan/datasets/c4_mini
2024-07-22 21:19:21,359 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:21,364 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:21,365 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:21,367 - root - INFO - Building llama3 70B with ModelArgs(dim=8192, n_layers=80, n_heads=64, n_kv_heads=8, vocab_size=128256, multiple_of=4096, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 21:19:21,518 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,523 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,523 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,625 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,630 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,630 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,652 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,655 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,657 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,657 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,660 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,660 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,685 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:21,685 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 21:19:21,695 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:21,696 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 21:19:21,716 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:21,716 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 21:19:21,773 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:21,773 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 21:19:21,779 - root - INFO - Model llama3 70B size: 70,553,706,496 total parameters
2024-07-22 21:19:21,779 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 21:19:21,850 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:21,852 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:21,864 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:21,865 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:21,870 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:21,876 - root - INFO - Training starts at step 1
2024-07-22 21:19:21,876 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:21,883 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:21,884 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:21,941 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:21,943 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:21,947 - root - INFO - Applied Tensor Parallelism to the model
2024-07-22 21:19:21,949 - root - INFO - Applied full activation checkpointing to the model
2024-07-22 21:19:22,274 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:22,284 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:22,303 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:22,367 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:22,383 - root - INFO - Compiled each TransformerBlock with torch.compile
2024-07-22 21:19:22,429 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:22,440 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:22,460 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:22,523 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:22,540 - root - INFO - Applied FSDP to the model
2024-07-22 21:19:22,761 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:22,773 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:22,775 - root - INFO - Training starts at step 1
2024-07-22 21:19:22,775 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:22,778 - root - INFO - Training starts at step 1
2024-07-22 21:19:22,778 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:22,956 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:22,961 - root - INFO - Training starts at step 1
2024-07-22 21:19:22,961 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:23,747 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:23,752 - root - INFO - Training starts at step 1
2024-07-22 21:19:23,752 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:23,825 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:23,830 - root - INFO - Training starts at step 1
2024-07-22 21:19:23,830 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:23,915 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:23,920 - root - INFO - Training starts at step 1
2024-07-22 21:19:23,920 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:24,315 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:24,320 - root - INFO - Training starts at step 1
2024-07-22 21:19:24,320 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 21:19:24,394 - root - INFO - GPU memory usage for model: 8.55GiB(10.81%)
2024-07-22 21:19:24,399 - root - INFO - Training starts at step 1
2024-07-22 21:19:24,399 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_inductor/lowering.py:1629: UserWarning: Torchinductor does not support code generation for complex operators. Performance may be worse than eager.
  warnings.warn(
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-22 21:22:11,789 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,788 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,789 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,788 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,788 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,789 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,788 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,790 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,789 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,789 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,790 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,788 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,789 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,790 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,788 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,790 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,877 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,878 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,879 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,879 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,878 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,877 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,879 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.60%
2024-07-22 21:22:11,879 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,879 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,878 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,878 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,879 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,878 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,878 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,879 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,879 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,885 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,886 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,886 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,886 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,885 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,885 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,886 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,885 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,886 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,885 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,886 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,886 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,885 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,886 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,887 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,885 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,915 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,914 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,914 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,914 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,915 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.59%
2024-07-22 21:22:11,915 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,915 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,914 - root - INFO - step:  1  loss: 12.2411  memory: 43.96GiB(55.57%)  wps: 12  mfu: 0.58%
2024-07-22 21:22:11,915 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,916 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,915 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,914 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,916 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,916 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,916 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:11,914 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.45%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 460  mfu: 22.38%
2024-07-22 21:22:51,885 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:51,884 - root - INFO - step: 10  loss: 11.3068  memory: 59.78GiB(75.56%)  wps: 461  mfu: 22.43%
2024-07-22 21:22:58,174 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,209 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,222 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,257 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,297 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,301 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,306 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,309 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,315 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,325 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,331 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,339 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,342 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,345 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,352 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,361 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,369 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,370 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,371 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,372 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,374 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,381 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,389 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,410 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,413 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,437 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,456 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,472 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,538 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,571 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,587 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:58,769 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:22:58,966 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 21:22:58,967 - root - INFO - Dumping traces at step 10
2024-07-22 21:22:59,002 - root - INFO - Finished dumping traces in 0.69 seconds
2024-07-22 21:22:59,001 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 21:22:59,004 - root - INFO - Finished dumping traces in 0.70 seconds
2024-07-22 21:22:59,015 - root - INFO - Finished dumping traces in 0.70 seconds
2024-07-22 21:22:59,014 - root - INFO - Finished dumping traces in 0.71 seconds
2024-07-22 21:22:59,015 - root - INFO - Finished dumping traces in 0.65 seconds
2024-07-22 21:22:59,015 - root - INFO - Finished dumping traces in 0.67 seconds
2024-07-22 21:22:59,022 - root - INFO - Finished dumping traces in 0.68 seconds
2024-07-22 21:22:59,022 - root - INFO - Finished dumping traces in 0.69 seconds
2024-07-22 21:22:59,021 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:22:59,033 - root - INFO - Finished dumping traces in 0.68 seconds
2024-07-22 21:22:59,033 - root - INFO - Finished dumping traces in 0.66 seconds
2024-07-22 21:22:59,032 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:22:59,038 - root - INFO - Finished dumping traces in 0.67 seconds
2024-07-22 21:22:59,038 - root - INFO - Finished dumping traces in 0.67 seconds
2024-07-22 21:22:59,037 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:22:59,053 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 21:22:59,054 - root - INFO - Finished dumping traces in 0.67 seconds
2024-07-22 21:22:59,055 - root - INFO - Finished dumping traces in 0.67 seconds
2024-07-22 21:22:59,059 - root - INFO - Finished dumping traces in 0.65 seconds
2024-07-22 21:22:59,059 - root - INFO - Finished dumping traces in 0.65 seconds
2024-07-22 21:22:59,060 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 21:22:59,062 - root - INFO - Finished dumping traces in 0.84 seconds
2024-07-22 21:22:59,091 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:22:59,121 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 21:22:59,137 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 21:22:59,142 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:22:59,142 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:22:59,158 - root - INFO - Finished dumping traces in 0.79 seconds
2024-07-22 21:22:59,522 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,769 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,769 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,768 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:27,767 - root - INFO - step: 20  loss:  9.7729  memory: 59.78GiB(75.56%)  wps: 571  mfu: 27.79%
2024-07-22 21:23:34,519 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,535 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,569 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,576 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,583 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,599 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,601 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,608 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,612 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,621 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,635 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,635 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,645 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,655 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,658 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,657 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,660 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,662 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,667 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,680 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,702 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,706 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,712 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,724 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,750 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,767 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,774 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,788 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,836 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,895 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,959 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:34,994 - root - INFO - Dumping traces at step 20
2024-07-22 21:23:35,073 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,078 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,102 - root - INFO - Finished dumping traces in 0.53 seconds
2024-07-22 21:23:35,121 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,121 - root - INFO - Finished dumping traces in 0.52 seconds
2024-07-22 21:23:35,127 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,153 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,164 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,165 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,185 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:23:35,191 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:23:35,191 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:23:35,203 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:23:35,207 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,213 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,213 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:23:35,219 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,221 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,227 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:23:35,244 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,245 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:23:35,252 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,260 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,262 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,279 - root - INFO - Finished dumping traces in 0.53 seconds
2024-07-22 21:23:35,316 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,326 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,337 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,379 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,445 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:23:35,499 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:23:35,525 - root - INFO - Finished dumping traces in 0.53 seconds
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,753 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:03,752 - root - INFO - step: 30  loss:  8.5126  memory: 59.78GiB(75.56%)  wps: 569  mfu: 27.71%
2024-07-22 21:24:10,822 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,889 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,892 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,914 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,927 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,932 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,947 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,949 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,951 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,963 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,985 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:10,994 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,001 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,014 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,018 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,020 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,027 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,033 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,039 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,048 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,055 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,063 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,066 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,068 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,086 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,091 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,091 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,105 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,111 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,132 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,136 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,304 - root - INFO - Dumping traces at step 30
2024-07-22 21:24:11,386 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,448 - root - INFO - Finished dumping traces in 0.53 seconds
2024-07-22 21:24:11,454 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,460 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,471 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:24:11,481 - root - INFO - Finished dumping traces in 0.53 seconds
2024-07-22 21:24:11,502 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,517 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,530 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:24:11,534 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:11,552 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:11,562 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:11,578 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,578 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:24:11,583 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,602 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:24:11,608 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:11,607 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,608 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:24:11,610 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,641 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,653 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,652 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,657 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,660 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,669 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:24:11,671 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:11,704 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:24:11,707 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:11,720 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:24:11,767 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 21:24:11,850 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 565  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,042 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 565  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,040 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:40,041 - root - INFO - step: 40  loss:  7.7261  memory: 59.78GiB(75.56%)  wps: 564  mfu: 27.48%
2024-07-22 21:24:47,103 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,128 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,145 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,154 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,160 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,170 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,200 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,216 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,217 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,225 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,226 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,234 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,234 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,251 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,301 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,305 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,311 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,331 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,334 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,341 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,340 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,344 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,345 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,364 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,386 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,402 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,425 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,443 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,448 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,503 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,515 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,573 - root - INFO - Dumping traces at step 40
2024-07-22 21:24:47,664 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,691 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,697 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:47,702 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,719 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,722 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,776 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:24:47,778 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,783 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:24:47,784 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,799 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:24:47,800 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:47,826 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:24:47,835 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:24:47,862 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,865 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,878 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:24:47,880 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,889 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,892 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,898 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,905 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:47,915 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:24:47,925 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:24:47,928 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:24:47,944 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:24:47,978 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:47,991 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:24:47,992 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:24:48,033 - root - INFO - Finished dumping traces in 0.53 seconds
2024-07-22 21:24:48,054 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:24:48,118 - root - INFO - Finished dumping traces in 0.54 seconds
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.07%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.07%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,272 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.06%
2024-07-22 21:25:23,273 - root - INFO - step: 50  loss:  7.6436  memory: 59.78GiB(75.56%)  wps: 474  mfu: 23.07%
2024-07-22 21:25:30,805 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,833 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,849 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,871 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,906 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,906 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,919 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,956 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,965 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,982 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,985 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:30,994 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,016 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,038 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,043 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,043 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,047 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,057 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,091 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,096 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,095 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,115 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,124 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,130 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,130 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,149 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,160 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,166 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,184 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,270 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,275 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,298 - root - INFO - Dumping traces at step 50
2024-07-22 21:25:31,397 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:25:31,397 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,460 - root - INFO - Finished dumping traces in 0.61 seconds
2024-07-22 21:25:31,491 - root - INFO - Finished dumping traces in 0.62 seconds
2024-07-22 21:25:31,494 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,507 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:25:31,557 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:25:31,579 - root - INFO - Finished dumping traces in 0.66 seconds
2024-07-22 21:25:31,585 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:25:31,585 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:25:31,585 - root - INFO - Finished dumping traces in 0.62 seconds
2024-07-22 21:25:31,587 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,593 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:25:31,612 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:25:31,625 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:25:31,625 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,629 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,637 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:25:31,660 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:25:31,691 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:25:31,699 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:25:31,708 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:25:31,714 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:25:31,722 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,722 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,734 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:25:31,774 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:25:31,777 - root - INFO - Finished dumping traces in 0.61 seconds
2024-07-22 21:25:31,829 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:25:31,851 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:25:31,858 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:25:31,866 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,990 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:25:59,989 - root - INFO - step: 60  loss:  7.4668  memory: 59.78GiB(75.56%)  wps: 558  mfu: 27.16%
2024-07-22 21:26:08,081 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,087 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,100 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,106 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,130 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,136 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,139 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,144 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,153 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,163 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,169 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,169 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,172 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,171 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,194 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,203 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,205 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,226 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,234 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,235 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,242 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,246 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,247 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,248 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,263 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,286 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,301 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,370 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,387 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,395 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,421 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,494 - root - INFO - Dumping traces at step 60
2024-07-22 21:26:08,828 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 21:26:08,829 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 21:26:08,871 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 21:26:08,871 - root - INFO - Finished dumping traces in 0.73 seconds
2024-07-22 21:26:08,872 - root - INFO - Finished dumping traces in 0.77 seconds
2024-07-22 21:26:08,871 - root - INFO - Finished dumping traces in 0.70 seconds
2024-07-22 21:26:08,876 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:26:08,877 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 21:26:08,877 - root - INFO - Finished dumping traces in 0.75 seconds
2024-07-22 21:26:08,881 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:26:08,882 - root - INFO - Finished dumping traces in 0.71 seconds
2024-07-22 21:26:08,882 - root - INFO - Finished dumping traces in 0.72 seconds
2024-07-22 21:26:08,885 - root - INFO - Finished dumping traces in 0.78 seconds
2024-07-22 21:26:08,886 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:26:08,886 - root - INFO - Finished dumping traces in 0.72 seconds
2024-07-22 21:26:08,890 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:26:08,890 - root - INFO - Finished dumping traces in 0.70 seconds
2024-07-22 21:26:08,893 - root - INFO - Finished dumping traces in 0.67 seconds
2024-07-22 21:26:08,894 - root - INFO - Finished dumping traces in 0.65 seconds
2024-07-22 21:26:08,895 - root - INFO - Finished dumping traces in 0.63 seconds
2024-07-22 21:26:08,896 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 21:26:08,913 - root - INFO - Finished dumping traces in 0.74 seconds
2024-07-22 21:26:08,919 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:26:08,942 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:26:08,956 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:26:08,984 - root - INFO - Finished dumping traces in 0.56 seconds
2024-07-22 21:26:08,991 - root - INFO - Finished dumping traces in 0.76 seconds
2024-07-22 21:26:09,013 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 21:26:09,015 - root - INFO - Finished dumping traces in 0.81 seconds
2024-07-22 21:26:09,046 - root - INFO - Finished dumping traces in 0.55 seconds
2024-07-22 21:26:09,089 - root - INFO - Finished dumping traces in 0.80 seconds
2024-07-22 21:26:09,135 - root - INFO - Finished dumping traces in 0.83 seconds
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,334 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 548  mfu: 26.70%
2024-07-22 21:26:37,335 - root - INFO - step: 70  loss:  7.3697  memory: 59.78GiB(75.56%)  wps: 549  mfu: 26.70%
2024-07-22 21:26:45,294 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,309 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,323 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,327 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,350 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,388 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,404 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,413 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,425 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,446 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,450 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,460 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,497 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,499 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,514 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,513 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,524 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,530 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,537 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,538 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,538 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,554 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,568 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,589 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,595 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,599 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,602 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,604 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,625 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,643 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,701 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,727 - root - INFO - Dumping traces at step 70
2024-07-22 21:26:45,898 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:26:45,921 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:26:45,926 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:26:45,926 - root - INFO - Finished dumping traces in 0.62 seconds
2024-07-22 21:26:45,974 - root - INFO - Finished dumping traces in 0.62 seconds
2024-07-22 21:26:45,977 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:26:45,997 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:26:45,997 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:26:46,036 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:26:46,043 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:26:46,115 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:26:46,115 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:26:46,120 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:26:46,119 - root - INFO - Finished dumping traces in 0.62 seconds
2024-07-22 21:26:46,152 - root - INFO - Finished dumping traces in 0.64 seconds
2024-07-22 21:26:46,151 - root - INFO - Finished dumping traces in 0.70 seconds
2024-07-22 21:26:46,156 - root - INFO - Finished dumping traces in 0.63 seconds
2024-07-22 21:26:46,157 - root - INFO - Finished dumping traces in 0.66 seconds
2024-07-22 21:26:46,161 - root - INFO - Finished dumping traces in 0.61 seconds
2024-07-22 21:26:46,161 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:26:46,163 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:26:46,173 - root - INFO - Finished dumping traces in 0.58 seconds
2024-07-22 21:26:46,193 - root - INFO - Finished dumping traces in 0.65 seconds
2024-07-22 21:26:46,197 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:26:46,196 - root - INFO - Finished dumping traces in 0.60 seconds
2024-07-22 21:26:46,211 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:26:46,238 - root - INFO - Finished dumping traces in 0.59 seconds
2024-07-22 21:26:46,238 - root - INFO - Finished dumping traces in 0.85 seconds
2024-07-22 21:26:46,272 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:26:46,298 - root - INFO - Finished dumping traces in 0.57 seconds
2024-07-22 21:26:46,323 - root - INFO - Finished dumping traces in 0.72 seconds
2024-07-22 21:26:46,396 - root - INFO - Finished dumping traces in 0.86 seconds

------------------------------------------------------------
Sender: LSF System <lsfadmin@p5-r19-n2>
Subject: Job 107760: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 21:18:55 2024
Job was executed on host(s) <1*p5-r19-n2>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 21:18:56 2024
                            <1*p1-r18-n2>
                            <1*p4-r24-n1>
                            <1*p5-r28-n3>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 21:18:56 2024
Terminated at Mon Jul 22 21:27:11 2024
Results reported at Mon Jul 22 21:27:11 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   23973.00 sec.
    Max Memory :                                 217047 MB
    Average Memory :                             116087.51 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              1100
    Max Threads :                                5663
    Run time :                                   495 sec.
    Turnaround time :                            496 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 22:38:22.807000 22989794170688 torch/distributed/run.py:793] 
W0722 22:38:22.807000 22989794170688 torch/distributed/run.py:793] *****************************************
W0722 22:38:22.807000 22989794170688 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:38:22.807000 22989794170688 torch/distributed/run.py:793] *****************************************
W0722 22:38:24.424000 22720490579776 torch/distributed/run.py:793] 
W0722 22:38:24.424000 22720490579776 torch/distributed/run.py:793] *****************************************
W0722 22:38:24.424000 22720490579776 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:38:24.424000 22720490579776 torch/distributed/run.py:793] *****************************************
2024-07-22 22:38:27,536 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,536 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,536 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,536 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,536 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,536 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,551 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:27,551 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,299 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,299 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,299 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,299 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,299 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,299 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,343 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:29,363 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:38:36,895 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:36,899 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:36,901 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:37,086 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:37,086 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:37,822 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:37,826 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:37,856 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:37,996 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:37,999 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,003 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,026 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:38,028 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,030 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,067 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,067 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,081 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:38,084 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,086 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,151 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:38,154 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,157 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,189 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,190 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,219 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,219 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,257 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:38,260 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,262 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,273 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,273 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,289 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:38,294 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,298 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,342 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,342 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,446 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,446 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,484 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,484 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:38,536 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:38,541 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:38,544 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:38,730 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:38,730 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,098 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,101 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,116 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,299 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,300 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,502 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,505 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,507 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,510 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,513 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,513 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,514 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,515 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,515 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,516 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,517 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,518 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,519 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,520 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,520 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:38:39,521 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,523 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:38:39,525 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:38:39,703 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,703 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,706 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,706 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,716 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,717 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,717 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,717 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,718 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,718 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:39,720 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:38:39,720 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:38:44,612 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:44,787 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:44,788 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:38:44,789 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:44,866 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:45,747 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:45,923 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:45,923 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:38:45,924 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:46,002 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:46,121 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:46,295 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:46,295 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:38:46,296 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:46,367 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:46,801 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:46,935 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:46,971 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:46,972 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:38:46,972 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:47,042 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:47,056 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:47,108 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:47,108 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:38:47,109 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:47,179 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:47,226 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:47,226 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:38:47,227 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:47,296 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:48,134 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:48,307 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:48,308 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:38:48,309 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:48,320 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:48,378 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:48,458 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:48,496 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:48,497 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:38:48,498 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:48,509 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:48,569 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:48,578 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:48,631 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:48,631 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:38:48,632 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:48,681 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:48,682 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:38:48,683 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:48,704 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:48,750 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:48,751 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:38:48,752 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:48,757 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:48,823 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:49,333 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:49,507 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:49,507 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:38:49,508 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:49,579 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:49,713 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:49,885 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:49,885 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:38:49,886 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:49,956 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:49,981 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:50,155 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:50,156 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:38:50,157 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:50,182 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:50,228 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:50,357 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:50,358 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:38:50,359 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:50,431 - root - INFO - Applied FSDP to the model
2024-07-22 22:38:50,598 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:38:50,774 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:38:50,774 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:38:50,775 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:38:50,848 - root - INFO - Applied FSDP to the model
2024-07-22 22:39:01,999 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:01,999 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:01,999 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:01,999 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,000 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,000 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,000 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,000 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,000 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,001 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,001 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,001 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,001 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,001 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,001 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,001 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,002 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,002 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,002 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,002 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,002 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,002 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,003 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,003 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,003 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,003 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,003 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,003 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,004 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,004 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,010 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,010 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,055 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,055 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,055 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,056 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,056 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,056 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,056 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,056 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,058 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2239
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Training starts at step 1
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:39:02,060 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 956  mfu: 5.60%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,582 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,582 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,581 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,581 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,581 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 955  mfu: 5.59%
2024-07-22 22:39:10,581 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,581 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,581 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,581 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,582 - root - INFO - step:  1  loss: 12.2508  memory: 41.59GiB(52.58%)  wps: 961  mfu: 5.63%
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:10,582 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,770 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,771 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:21,772 - root - INFO - step: 10  loss: 10.7822  memory: 41.59GiB(52.58%)  wps: 6,590  mfu: 38.59%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,567  mfu: 38.46%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,568  mfu: 38.46%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,567  mfu: 38.46%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,566  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,565  mfu: 38.45%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,567  mfu: 38.46%
2024-07-22 22:39:34,256 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,568  mfu: 38.46%
2024-07-22 22:39:34,257 - root - INFO - step: 20  loss:  9.1296  memory: 41.59GiB(52.58%)  wps: 6,567  mfu: 38.46%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:39:46,804 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.27%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:39:46,804 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:39:46,804 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:46,803 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:39:46,804 - root - INFO - step: 30  loss:  8.0269  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:39:59,390 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,512  mfu: 38.13%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,512  mfu: 38.13%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,515  mfu: 38.15%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,512  mfu: 38.13%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,512  mfu: 38.13%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,514  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,513  mfu: 38.14%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,512  mfu: 38.13%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,516  mfu: 38.16%
2024-07-22 22:39:59,391 - root - INFO - step: 40  loss:  7.3708  memory: 41.59GiB(52.58%)  wps: 6,514  mfu: 38.15%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.04%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,499  mfu: 38.06%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,498  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,498  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,498  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,499  mfu: 38.06%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,499  mfu: 38.06%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,498  mfu: 38.05%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,500  mfu: 38.06%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,499  mfu: 38.06%
2024-07-22 22:40:12,008 - root - INFO - step: 50  loss:  7.1197  memory: 41.59GiB(52.58%)  wps: 6,497  mfu: 38.05%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,491  mfu: 38.01%
2024-07-22 22:40:24,643 - root - INFO - step: 60  loss:  6.9219  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,491  mfu: 38.01%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,488  mfu: 37.99%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,492  mfu: 38.02%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:37,278 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:37,279 - root - INFO - step: 70  loss:  6.8098  memory: 41.59GiB(52.58%)  wps: 6,491  mfu: 38.01%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,491  mfu: 38.01%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.01%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,492  mfu: 38.02%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,492  mfu: 38.02%
2024-07-22 22:40:49,910 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,490  mfu: 38.00%
2024-07-22 22:40:49,911 - root - INFO - step: 80  loss:  6.6378  memory: 41.59GiB(52.58%)  wps: 6,493  mfu: 38.02%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,486  mfu: 37.98%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,487  mfu: 37.98%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,486  mfu: 37.98%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.98%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,551 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,486  mfu: 37.98%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,485  mfu: 37.97%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,489  mfu: 38.00%
2024-07-22 22:41:02,552 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,486  mfu: 37.98%
2024-07-22 22:41:02,553 - root - INFO - step: 90  loss:  6.5812  memory: 41.59GiB(52.58%)  wps: 6,486  mfu: 37.98%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,097  mfu: 35.70%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,097  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,097  mfu: 35.70%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,097  mfu: 35.71%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,100  mfu: 35.72%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,099  mfu: 35.71%
2024-07-22 22:41:15,995 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,101  mfu: 35.73%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,099  mfu: 35.71%
2024-07-22 22:41:15,996 - root - INFO - step: 100  loss:  6.4209  memory: 41.59GiB(52.58%)  wps: 6,098  mfu: 35.71%
2024-07-22 22:41:18,323 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,350 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,351 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,361 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,367 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,368 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,370 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,383 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,422 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,422 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,431 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,442 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,455 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,463 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,501 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,509 - root - INFO - Dumping traces at step 100
2024-07-22 22:41:18,564 - root - INFO - Finished dumping traces in 0.24 seconds
2024-07-22 22:41:18,611 - root - INFO - Finished dumping traces in 0.26 seconds
2024-07-22 22:41:18,622 - root - INFO - Finished dumping traces in 0.27 seconds
2024-07-22 22:41:18,624 - root - INFO - Finished dumping traces in 0.26 seconds
2024-07-22 22:41:18,625 - root - INFO - Finished dumping traces in 0.26 seconds
2024-07-22 22:41:18,634 - root - INFO - Finished dumping traces in 0.26 seconds
2024-07-22 22:41:18,652 - root - INFO - Finished dumping traces in 0.27 seconds
2024-07-22 22:41:18,653 - root - INFO - Finished dumping traces in 0.29 seconds
2024-07-22 22:41:18,664 - root - INFO - Finished dumping traces in 0.24 seconds
2024-07-22 22:41:18,664 - root - INFO - Finished dumping traces in 0.24 seconds
2024-07-22 22:41:18,678 - root - INFO - Finished dumping traces in 0.24 seconds
2024-07-22 22:41:18,691 - root - INFO - Finished dumping traces in 0.26 seconds
2024-07-22 22:41:18,694 - root - INFO - Finished dumping traces in 0.24 seconds
2024-07-22 22:41:18,709 - root - INFO - Finished dumping traces in 0.25 seconds
2024-07-22 22:41:18,755 - root - INFO - Finished dumping traces in 0.25 seconds
2024-07-22 22:41:18,758 - root - INFO - Finished dumping traces in 0.25 seconds
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,356  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,356  mfu: 31.37%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,357  mfu: 31.37%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,355  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,356  mfu: 31.37%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,356  mfu: 31.36%
2024-07-22 22:41:31,302 - root - INFO - step: 110  loss:  6.3561  memory: 41.59GiB(52.58%)  wps: 5,356  mfu: 31.37%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,528  mfu: 38.22%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,527  mfu: 38.22%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,528  mfu: 38.22%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,528  mfu: 38.23%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,529  mfu: 38.24%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,527  mfu: 38.22%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,529  mfu: 38.23%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,527  mfu: 38.22%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,527  mfu: 38.22%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,527  mfu: 38.22%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,529  mfu: 38.23%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,529  mfu: 38.23%
2024-07-22 22:41:43,861 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,529  mfu: 38.23%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,527  mfu: 38.22%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:41:43,862 - root - INFO - step: 120  loss:  6.3096  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:41:56,416 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:41:56,417 - root - INFO - step: 130  loss:  6.2686  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,530  mfu: 38.24%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.24%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,532  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.28%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.25%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:42:08,970 - root - INFO - step: 140  loss:  6.1985  memory: 41.59GiB(52.58%)  wps: 6,531  mfu: 38.25%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.28%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,537  mfu: 38.28%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,538  mfu: 38.28%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,539  mfu: 38.29%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,537  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,537  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,542  mfu: 38.31%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,538  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,537  mfu: 38.28%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,539  mfu: 38.29%
2024-07-22 22:42:21,511 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,538  mfu: 38.28%
2024-07-22 22:42:21,512 - root - INFO - step: 150  loss:  6.1689  memory: 41.59GiB(52.58%)  wps: 6,540  mfu: 38.30%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,550  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,550  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,550  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,551  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,552  mfu: 38.37%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,550  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,551  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,553  mfu: 38.37%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,550  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,552  mfu: 38.37%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,551  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,551  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,553  mfu: 38.37%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,551  mfu: 38.36%
2024-07-22 22:42:34,027 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,553  mfu: 38.37%
2024-07-22 22:42:34,028 - root - INFO - step: 160  loss:  6.0946  memory: 41.59GiB(52.58%)  wps: 6,552  mfu: 38.37%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.27%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,535  mfu: 38.27%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,534  mfu: 38.26%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:42:46,574 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,533  mfu: 38.26%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.27%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.27%
2024-07-22 22:42:46,575 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,537  mfu: 38.28%
2024-07-22 22:42:46,577 - root - INFO - step: 170  loss:  6.0163  memory: 41.59GiB(52.58%)  wps: 6,536  mfu: 38.27%

------------------------------------------------------------
Sender: LSF System <lsfadmin@p4-r24-n1>
Subject: Job 107834: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 22:38:16 2024
Job was executed on host(s) <1*p4-r24-n1>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 22:38:16 2024
                            <1*p4-r05-n2>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 22:38:16 2024
Terminated at Mon Jul 22 22:42:51 2024
Results reported at Mon Jul 22 22:42:51 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   6798.00 sec.
    Max Memory :                                 31934 MB
    Average Memory :                             20803.72 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                636
    Run time :                                   277 sec.
    Turnaround time :                            275 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 22:43:23.562000 23007028328256 torch/distributed/run.py:793] 
W0722 22:43:23.562000 23007028328256 torch/distributed/run.py:793] *****************************************
W0722 22:43:23.562000 23007028328256 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:43:23.562000 23007028328256 torch/distributed/run.py:793] *****************************************
W0722 22:43:24.095000 23315767920448 torch/distributed/run.py:793] 
W0722 22:43:24.095000 23315767920448 torch/distributed/run.py:793] *****************************************
W0722 22:43:24.095000 23315767920448 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:43:24.095000 23315767920448 torch/distributed/run.py:793] *****************************************
2024-07-22 22:43:27,757 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,757 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,757 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,757 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,757 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,757 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,775 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:27,775 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,207 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,207 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,207 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,207 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,207 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,209 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,211 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:30,211 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:43:36,975 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:36,978 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:36,980 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:37,167 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:37,167 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:37,765 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:37,768 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:37,782 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:37,963 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:37,964 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:38,165 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:38,169 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:38,173 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:38,273 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:38,277 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:38,279 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:38,326 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:38,329 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:38,333 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:38,361 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:38,361 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:38,464 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:38,464 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:38,519 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:38,519 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:38,587 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:38,590 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:38,591 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:38,592 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:38,593 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:38,594 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:38,596 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:38,597 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:38,601 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:38,777 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:38,778 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:38,784 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:38,784 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:38,785 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:38,785 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:39,471 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:39,475 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:39,513 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:39,739 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:39,740 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,064 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,067 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,081 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,103 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,106 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,109 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,225 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,232 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,246 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,265 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,266 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,292 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,293 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,430 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,430 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,517 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,522 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,525 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,534 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,538 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,541 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,553 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,554 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,560 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,566 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:43:40,570 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:43:40,575 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:43:40,718 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,718 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,737 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,737 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,749 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,750 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:40,761 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:43:40,761 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:43:43,842 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:44,012 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:44,013 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:43:44,014 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:44,084 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:45,476 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:45,651 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:45,651 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:45,651 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:43:45,652 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:45,677 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:45,722 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:45,822 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:45,822 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:43:45,823 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:45,851 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:45,852 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:43:45,853 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:45,893 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:45,923 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:46,282 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:46,315 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:46,455 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:46,456 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:43:46,457 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:46,490 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:46,491 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:43:46,492 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:46,528 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:46,563 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:46,903 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:47,074 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:47,074 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:43:47,075 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:47,144 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:47,506 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:47,680 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:47,680 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:43:47,681 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:47,753 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:48,598 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:48,773 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:48,773 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:43:48,774 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:48,853 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:48,856 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:49,030 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:49,031 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:43:49,032 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:49,104 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:49,189 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:49,197 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:49,203 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:49,353 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:49,361 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:49,361 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:43:49,362 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:49,370 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:49,370 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:43:49,371 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:49,371 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:49,374 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:49,375 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:43:49,376 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:49,432 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:49,441 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:49,446 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:49,527 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:49,528 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:43:49,528 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:49,542 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:49,543 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:43:49,544 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:49,599 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:49,614 - root - INFO - Applied FSDP to the model
2024-07-22 22:43:50,151 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:43:50,324 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:43:50,327 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:43:50,327 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:43:50,399 - root - INFO - Applied FSDP to the model
2024-07-22 22:44:02,661 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,661 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,662 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,662 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,662 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,662 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,662 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,662 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,663 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,663 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,663 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,663 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,664 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,664 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,664 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,664 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,664 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,664 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,664 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,664 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,664 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,665 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,665 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,665 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,665 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,665 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,665 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,665 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,665 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,665 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,665 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,665 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,702 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,702 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,702 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,703 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,703 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,703 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,704 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,706 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:44:02,706 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,707 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,707 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,707 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,707 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,707 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,707 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,707 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,707 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,707 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,707 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,707 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,707 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,708 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2244
2024-07-22 22:44:02,709 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,709 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:44:02,716 - root - INFO - Training starts at step 1
2024-07-22 22:44:02,716 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/autograd/graph.py:799: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:667.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,854 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.31%
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,855 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 735  mfu: 4.30%
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,854 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:13,854 - root - INFO - step:  1  loss: 12.2205  memory: 71.56GiB(90.46%)  wps: 732  mfu: 4.29%
2024-07-22 22:44:13,855 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,315 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:44:53,314 - root - INFO - step: 10  loss: 10.6731  memory: 71.56GiB(90.46%)  wps: 1,868  mfu: 10.94%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:45:37,739 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,845  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:45:37,738 - root - INFO - step: 20  loss:  9.0714  memory: 71.56GiB(90.46%)  wps: 1,846  mfu: 10.81%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,848  mfu: 10.82%
2024-07-22 22:46:22,115 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,848  mfu: 10.82%
2024-07-22 22:46:22,115 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,848  mfu: 10.82%
2024-07-22 22:46:22,115 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,848  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,848  mfu: 10.82%
2024-07-22 22:46:22,114 - root - INFO - step: 30  loss:  8.0527  memory: 71.56GiB(90.46%)  wps: 1,847  mfu: 10.82%
2024-07-22 22:47:06,380 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,853  mfu: 10.85%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.85%
2024-07-22 22:47:06,381 - root - INFO - step: 40  loss:  7.4101  memory: 71.56GiB(90.46%)  wps: 1,852  mfu: 10.84%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,576 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:47:50,575 - root - INFO - step: 50  loss:  7.1460  memory: 71.56GiB(90.46%)  wps: 1,855  mfu: 10.86%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,863  mfu: 10.91%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,606 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%
2024-07-22 22:48:34,605 - root - INFO - step: 60  loss:  6.9745  memory: 71.56GiB(90.46%)  wps: 1,862  mfu: 10.90%

------------------------------------------------------------
Sender: LSF System <lsfadmin@p4-r20-n4>
Subject: Job 107835: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 22:43:16 2024
Job was executed on host(s) <1*p4-r20-n4>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 22:43:16 2024
                            <1*p4-r24-n1>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 22:43:16 2024
Terminated at Mon Jul 22 22:48:49 2024
Results reported at Mon Jul 22 22:48:49 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with signal termination: 9.

Resource usage summary:

    CPU time :                                   8709.00 sec.
    Max Memory :                                 27372 MB
    Average Memory :                             20799.98 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              24
    Max Threads :                                604
    Run time :                                   332 sec.
    Turnaround time :                            333 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 22:48:59.550000 22435977951040 torch/distributed/run.py:793] 
W0722 22:48:59.550000 22435977951040 torch/distributed/run.py:793] *****************************************
W0722 22:48:59.550000 22435977951040 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:48:59.550000 22435977951040 torch/distributed/run.py:793] *****************************************
W0722 22:49:00.277000 23380498777920 torch/distributed/run.py:793] 
W0722 22:49:00.277000 23380498777920 torch/distributed/run.py:793] *****************************************
W0722 22:49:00.277000 23380498777920 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:49:00.277000 23380498777920 torch/distributed/run.py:793] *****************************************
2024-07-22 22:49:02,882 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,883 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,888 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,889 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,890 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,893 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,896 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:02,915 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,060 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,060 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,060 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,060 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,062 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,074 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,074 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:03,075 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:49:11,510 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:11,513 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:11,526 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:11,706 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:11,706 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:12,560 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:12,563 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:12,576 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:12,758 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:12,759 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:12,950 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:12,953 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:12,967 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,050 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,055 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,062 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,065 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,067 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,071 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,089 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,092 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,094 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,144 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,147 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,150 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,150 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,161 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,223 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,226 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,240 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,252 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,252 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,256 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,256 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,256 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,258 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,261 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,261 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,263 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,264 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,264 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,266 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,266 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,267 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,268 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,269 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,279 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,280 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,347 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,347 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,357 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,360 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,365 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,368 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,373 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,382 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,410 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,413 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,426 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:49:13,426 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,429 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:49:13,431 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:49:13,435 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,436 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,450 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,451 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,452 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,452 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,452 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,452 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,453 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,453 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,562 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,562 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,576 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,576 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,613 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,613 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:13,615 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:49:13,615 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:49:17,372 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:17,546 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:17,547 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:49:17,548 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:17,618 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:20,442 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:20,614 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:20,615 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:49:20,616 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:20,640 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:20,686 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:20,814 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:20,814 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:49:20,815 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:20,882 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:20,885 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:21,054 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:21,054 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:49:21,055 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:21,124 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:21,125 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:21,203 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:21,294 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:21,295 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:49:21,296 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:21,365 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:21,378 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:21,379 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:49:21,380 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:21,450 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:22,113 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:22,286 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:22,287 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:49:22,288 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:22,360 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:22,413 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:22,434 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:22,517 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:22,586 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:22,587 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:49:22,588 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:22,609 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:22,609 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:49:22,610 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:22,660 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:22,680 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:22,690 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:22,690 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:49:22,691 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:22,763 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:22,819 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:22,995 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:22,995 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:49:22,996 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:22,998 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:23,068 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:23,120 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:23,170 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:23,171 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:49:23,171 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:23,242 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:23,292 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:23,293 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:49:23,294 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:23,317 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:23,363 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:23,471 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:23,488 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:23,489 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:49:23,489 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:23,558 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:23,643 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:23,643 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:49:23,644 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:23,718 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:24,393 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:49:24,565 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:49:24,565 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:49:24,566 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:49:24,636 - root - INFO - Applied FSDP to the model
2024-07-22 22:49:35,673 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,673 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,674 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,675 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,676 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,676 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,682 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,682 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,721 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,721 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,721 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,722 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,722 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,723 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,723 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,723 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,723 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,724 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,724 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,724 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,724 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,724 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,724 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,724 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,724 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,725 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,725 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,725 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,725 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,725 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,725 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,725 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,725 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,726 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,726 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,726 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,726 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,726 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,727 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,727 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,727 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,727 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,727 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,727 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:49:35,727 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,727 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,727 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,727 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,727 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,728 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,728 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,728 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,728 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,728 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,728 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,728 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,728 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2249
2024-07-22 22:49:35,729 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,729 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,729 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,729 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,730 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:49:35,730 - root - INFO - Training starts at step 1
2024-07-22 22:49:35,730 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank0]:     pred = model(input_ids)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank0]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank0]:     ret = function(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank0]:     return self_._op(*args, **kwargs)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank1]:     pred = model(input_ids)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank1]:     h = layer(h, self.freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank1]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank1]:     ret = function(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank1]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank1]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank1]:     return self_._op(*args, **kwargs)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank5]:     pred = model(input_ids)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank5]:     h = layer(h, self.freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank5]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank5]:     ret = function(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank5]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank5]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank5]:     out = func(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank5]:     return self_._op(*args, **kwargs)
[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 5 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank3]:     pred = model(input_ids)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank3]:     h = layer(h, self.freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank3]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank3]:     ret = function(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank3]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank3]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank3]:     return self_._op(*args, **kwargs)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank4]:     pred = model(input_ids)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank4]:     h = layer(h, self.freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank4]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank4]:     ret = function(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank4]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank4]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank4]:     out = func(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank4]:     return self_._op(*args, **kwargs)
[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank6]:     pred = model(input_ids)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank6]:     h = layer(h, self.freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank6]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank6]:     ret = function(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank6]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank6]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank6]:     return torch._C._nn.silu(input)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank6]:     out = func(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank6]:     return self_._op(*args, **kwargs)
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 60.88 MiB is free. Including non-PyTorch memory, this process has 79.03 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank7]:     pred = model(input_ids)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank7]:     h = layer(h, self.freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank7]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank7]:     ret = function(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank7]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank7]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank7]:     return torch._C._nn.silu(input)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank7]:     out = func(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank7]:     return self_._op(*args, **kwargs)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 7 has a total capacity of 79.11 GiB of which 44.88 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank2]:     pred = model(input_ids)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank2]:     h = layer(h, self.freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank2]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank2]:     ret = function(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank2]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank2]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank2]:     return self_._op(*args, **kwargs)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank10]: Traceback (most recent call last):
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank10]:     main(config)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank10]:     return f(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank10]:     pred = model(input_ids)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank10]:     result = forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank10]:     h = layer(h, self.freqs_cis)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank10]:     result = forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank10]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank10]:     return disable_fn(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank10]:     return fn(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank10]:     ret = function(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank10]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank10]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank10]:     out = func(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank10]:     return self_._op(*args, **kwargs)
[rank10]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank9]: Traceback (most recent call last):
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank9]:     main(config)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank9]:     return f(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank9]:     pred = model(input_ids)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank9]:     result = forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank9]:     h = layer(h, self.freqs_cis)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank9]:     result = forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank9]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank9]:     return disable_fn(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank9]:     return fn(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank9]:     ret = function(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank9]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank9]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank9]:     out = func(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank9]:     return self_._op(*args, **kwargs)
[rank9]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank8]: Traceback (most recent call last):
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank8]:     main(config)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank8]:     return f(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank8]:     pred = model(input_ids)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank8]:     result = forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank8]:     h = layer(h, self.freqs_cis)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank8]:     result = forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank8]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank8]:     return disable_fn(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank8]:     return fn(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank8]:     ret = function(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank8]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank8]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank8]:     out = func(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank8]:     return self_._op(*args, **kwargs)
[rank8]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank14]: Traceback (most recent call last):
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank14]:     main(config)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank14]:     return f(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank14]:     pred = model(input_ids)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank14]:     result = forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank14]:     h = layer(h, self.freqs_cis)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank14]:     result = forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank14]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank14]:     return disable_fn(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank14]:     return fn(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank14]:     ret = function(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank14]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank14]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank14]:     out = func(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank14]:     return self_._op(*args, **kwargs)
[rank14]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank15]:     main(config)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank15]:     return f(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank15]:     pred = model(input_ids)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank15]:     result = forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank15]:     h = layer(h, self.freqs_cis)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank15]:     result = forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank15]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank15]:     return disable_fn(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank15]:     return fn(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank15]:     ret = function(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank15]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank15]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank15]:     return torch._C._nn.silu(input)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank15]:     out = func(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank15]:     return self_._op(*args, **kwargs)
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 7 has a total capacity of 79.11 GiB of which 44.88 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank11]: Traceback (most recent call last):
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank11]:     main(config)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank11]:     return f(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank11]:     pred = model(input_ids)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank11]:     result = forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank11]:     h = layer(h, self.freqs_cis)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank11]:     result = forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank11]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank11]:     return disable_fn(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank11]:     return fn(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank11]:     ret = function(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank11]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank11]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank11]:     out = func(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank11]:     return self_._op(*args, **kwargs)
[rank11]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank12]: Traceback (most recent call last):
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank12]:     main(config)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank12]:     return f(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank12]:     pred = model(input_ids)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank12]:     result = forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank12]:     h = layer(h, self.freqs_cis)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank12]:     result = forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank12]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank12]:     return disable_fn(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank12]:     return fn(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank12]:     ret = function(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank12]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank12]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank12]:     return torch._C._nn.silu(input)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank12]:     out = func(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank12]:     return self_._op(*args, **kwargs)
[rank12]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 60.88 MiB is free. Including non-PyTorch memory, this process has 79.03 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank13]: Traceback (most recent call last):
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank13]:     main(config)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank13]:     return f(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank13]:     pred = model(input_ids)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank13]:     result = forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank13]:     h = layer(h, self.freqs_cis)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank13]:     result = forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank13]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank13]:     return disable_fn(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank13]:     return fn(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank13]:     ret = function(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank13]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank13]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank13]:     out = func(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank13]:     return self_._op(*args, **kwargs)
[rank13]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 5 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126666 closing signal SIGTERM
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126667 closing signal SIGTERM
W0722 22:49:43.454000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024346 closing signal SIGTERM
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126668 closing signal SIGTERM
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126669 closing signal SIGTERM
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126671 closing signal SIGTERM
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126672 closing signal SIGTERM
W0722 22:49:43.454000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 126673 closing signal SIGTERM
W0722 22:49:43.457000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024347 closing signal SIGTERM
W0722 22:49:43.458000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024349 closing signal SIGTERM
W0722 22:49:43.458000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024350 closing signal SIGTERM
W0722 22:49:43.458000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024351 closing signal SIGTERM
W0722 22:49:43.458000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024352 closing signal SIGTERM
W0722 22:49:43.458000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4024353 closing signal SIGTERM
E0722 22:49:45.520000 23380498777920 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 4 (pid: 126670) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 22:49:45.529000 23380498777920 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_47bqd0fv/none_8706dgf7/attempt_0/4/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_22:49:41
  host      : p4-r24-n1.bluevela.rmf.ibm.com
  rank      : 12 (local_rank: 4)
  exitcode  : 1 (pid: 126670)
  error_file: /tmp/torchelastic_47bqd0fv/none_8706dgf7/attempt_0/4/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
      return torch._C._nn.silu(input)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 60.88 MiB is free. Including non-PyTorch memory, this process has 79.03 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================
E0722 22:49:45.687000 22435977951040 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 2 (pid: 4024348) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 22:49:45.696000 22435977951040 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_0rt8shlx/none_n9mvl4pb/attempt_0/2/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_22:49:41
  host      : p4-r05-n2.bluevela.rmf.ibm.com
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 4024348)
  error_file: /tmp/torchelastic_0rt8shlx/none_n9mvl4pb/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p4-r05-n2>
Subject: Job 107836: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 22:48:53 2024
Job was executed on host(s) <1*p4-r05-n2>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 22:48:53 2024
                            <1*p4-r24-n1>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 22:48:53 2024
Terminated at Mon Jul 22 22:49:47 2024
Results reported at Mon Jul 22 22:49:47 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   462.00 sec.
    Max Memory :                                 11748 MB
    Average Memory :                             4201.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   58 sec.
    Turnaround time :                            54 sec.

The output (if any) is above this job summary.

xargs: echo: terminated by signal 13
xargs: echo: terminated by signal 13
W0722 22:50:22.110000 22892387923776 torch/distributed/run.py:793] 
W0722 22:50:22.110000 22892387923776 torch/distributed/run.py:793] *****************************************
W0722 22:50:22.110000 22892387923776 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:50:22.110000 22892387923776 torch/distributed/run.py:793] *****************************************
W0722 22:50:22.984000 22462552614720 torch/distributed/run.py:793] 
W0722 22:50:22.984000 22462552614720 torch/distributed/run.py:793] *****************************************
W0722 22:50:22.984000 22462552614720 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0722 22:50:22.984000 22462552614720 torch/distributed/run.py:793] *****************************************
2024-07-22 22:50:25,735 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,736 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,743 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,748 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,764 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,769 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,770 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,771 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,789 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,790 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,799 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,800 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,801 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,803 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,806 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:25,816 - root - INFO - Starting job: Llama 3 8B training
2024-07-22 22:50:34,778 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:34,781 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:34,794 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:34,977 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:34,977 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:35,222 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,225 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,240 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,420 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:35,420 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:35,710 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,714 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,715 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,718 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,721 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,727 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,767 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,773 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,778 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,783 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,796 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,799 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,802 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,807 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,821 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,839 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:35,842 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:35,856 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:35,908 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:35,908 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:35,913 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:35,913 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:35,986 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:35,986 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:35,989 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:35,989 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,011 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,011 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,011 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,014 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,016 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,019 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,022 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,037 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,045 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,045 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,199 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,202 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,202 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,204 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,218 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,224 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,224 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,236 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,241 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,244 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,256 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,265 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,272 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,404 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,404 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,439 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,439 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,465 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,465 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,467 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,471 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,486 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,507 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,509 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,512 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,674 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,674 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,695 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,695 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:36,764 - root - WARNING - ENV[TORCH_NCCL_ASYNC_ERROR_HANDLING] = 1 will be overridden to 3 based on job config
2024-07-22 22:50:36,769 - root - INFO - Building 1-D device mesh with ['dp'], [16]
2024-07-22 22:50:36,773 - root - INFO - Building tiktoken tokenizer locally from ./torchtitan/datasets/tokenizer/original/tokenizer.model
2024-07-22 22:50:36,956 - root - INFO - TikTokenizer built: #words 128256, BOS ID 128000, EOS ID 128001
2024-07-22 22:50:36,956 - root - INFO - Preparing c4 dataset from allenai/c4
2024-07-22 22:50:45,768 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:45,938 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:45,939 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:50:45,940 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:46,013 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:46,092 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:46,264 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:46,264 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:50:46,265 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:46,335 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:46,949 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:47,126 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:47,128 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:50:47,129 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:47,204 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:49,733 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:49,904 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:49,905 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:50:49,905 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:49,977 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:50,465 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:50,533 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:50,638 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:50,639 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:50:50,640 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:50,706 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:50,707 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:50:50,708 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:50,720 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:50,780 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:51,663 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:51,839 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:51,840 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:50:51,841 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:51,908 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:51,913 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:52,081 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:52,082 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (4) with 79.11GiB memory
2024-07-22 22:50:52,082 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:52,153 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:52,319 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:52,490 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:52,491 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (0) with 79.11GiB memory
2024-07-22 22:50:52,491 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:52,561 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:53,104 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:53,243 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:53,274 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:53,274 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (6) with 79.11GiB memory
2024-07-22 22:50:53,275 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:53,345 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:53,415 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:53,416 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:50:53,416 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:53,486 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:54,513 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:54,683 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:54,683 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (2) with 79.11GiB memory
2024-07-22 22:50:54,684 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:54,753 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:55,435 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:55,564 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:55,609 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:55,609 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (5) with 79.11GiB memory
2024-07-22 22:50:55,610 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:55,681 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:55,741 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:55,742 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (1) with 79.11GiB memory
2024-07-22 22:50:55,743 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:55,813 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:56,281 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:56,454 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:56,454 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (7) with 79.11GiB memory
2024-07-22 22:50:56,455 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:56,526 - root - INFO - Applied FSDP to the model
2024-07-22 22:50:56,882 - root - INFO - Building llama3 8B with ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=8, vocab_size=128256, multiple_of=1024, ffn_dim_multiplier=1.3, norm_eps=1e-05, rope_theta=500000, max_batch_size=32, max_seq_len=8192, depth_init=True, norm_type='rmsnorm')
2024-07-22 22:50:57,058 - root - INFO - Model llama3 8B size: 8,030,261,248 total parameters
2024-07-22 22:50:57,058 - root - INFO - GPU capacity: NVIDIA H100 80GB HBM3 (3) with 79.11GiB memory
2024-07-22 22:50:57,059 - root - INFO - Applied selective activation checkpointing to the model
2024-07-22 22:50:57,130 - root - INFO - Applied FSDP to the model
2024-07-22 22:51:07,678 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,678 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,679 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,679 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,680 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,680 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,680 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,681 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,681 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,681 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,681 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,681 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,682 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,682 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,682 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,684 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,685 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,685 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,686 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,686 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,690 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,691 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,691 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,692 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,692 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,692 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,693 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,693 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,694 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,694 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,694 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,694 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,697 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,697 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,697 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,697 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,697 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,697 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,698 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,698 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,698 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,698 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,698 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,698 - root - INFO - GPU memory usage for model: 1.92GiB(2.42%)
2024-07-22 22:51:07,698 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,699 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,699 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,699 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,699 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,699 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,699 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,699 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,699 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,699 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,699 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,700 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,700 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,700 - root - INFO - Metrics logging active. Tensorboard logs will be saved at ./outputs/tb/20240722-2251
2024-07-22 22:51:07,700 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,700 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,700 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,700 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
2024-07-22 22:51:07,701 - root - INFO - Training starts at step 1
2024-07-22 22:51:07,701 - root - INFO - Profiling active. Traces will be saved at ./outputs/profile_trace
[rank8]: Traceback (most recent call last):
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank8]:     main(config)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank8]:     return f(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank8]:     pred = model(input_ids)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank8]:     result = forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank8]:     h = layer(h, self.freqs_cis)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank8]:     result = forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank8]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank8]:     return disable_fn(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank8]:     return fn(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank8]:     ret = function(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank8]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank8]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank8]:     out = func(*args, **kwargs)
[rank8]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank8]:     return self_._op(*args, **kwargs)
[rank8]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank2]:     main(config)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank2]:     return f(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank2]:     pred = model(input_ids)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank2]:     h = layer(h, self.freqs_cis)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank2]:     result = forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank2]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank2]:     ret = function(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank2]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank2]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank2]:     return self_._op(*args, **kwargs)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]: Traceback (most recent call last):
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank4]:     main(config)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank4]:     return f(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank4]:     pred = model(input_ids)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank4]:     h = layer(h, self.freqs_cis)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank4]:     result = forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank4]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank4]:     ret = function(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank4]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank4]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank4]:     out = func(*args, **kwargs)
[rank4]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank4]:     return self_._op(*args, **kwargs)
[rank4]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: Traceback (most recent call last):
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank6]:     main(config)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank6]:     return f(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank6]:     pred = model(input_ids)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank6]:     h = layer(h, self.freqs_cis)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank6]:     result = forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank6]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank6]:     ret = function(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank6]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank6]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank6]:     return torch._C._nn.silu(input)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank6]:     out = func(*args, **kwargs)
[rank6]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank6]:     return self_._op(*args, **kwargs)
[rank6]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 60.88 MiB is free. Including non-PyTorch memory, this process has 79.03 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank3]:     main(config)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank3]:     return f(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank3]:     pred = model(input_ids)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank3]:     h = layer(h, self.freqs_cis)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank3]:     result = forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank3]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank3]:     ret = function(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank3]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank3]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank3]:     return self_._op(*args, **kwargs)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank0]:     main(config)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank0]:     return f(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank0]:     pred = model(input_ids)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank0]:     h = layer(h, self.freqs_cis)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank0]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank0]:     ret = function(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank0]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank0]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank0]:     return self_._op(*args, **kwargs)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank5]:     main(config)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank5]:     return f(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank5]:     pred = model(input_ids)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank5]:     h = layer(h, self.freqs_cis)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank5]:     result = forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank5]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank5]:     ret = function(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank5]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank5]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank5]:     out = func(*args, **kwargs)
[rank5]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank5]:     return self_._op(*args, **kwargs)
[rank5]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 5 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank1]:     main(config)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank1]:     return f(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank1]:     pred = model(input_ids)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank1]:     h = layer(h, self.freqs_cis)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank1]:     result = forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank1]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank1]:     ret = function(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank1]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank1]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank1]:     return self_._op(*args, **kwargs)
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank7]:     main(config)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank7]:     return f(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank7]:     pred = model(input_ids)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank7]:     h = layer(h, self.freqs_cis)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank7]:     result = forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank7]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank7]:     ret = function(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank7]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank7]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank7]:     return torch._C._nn.silu(input)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank7]:     out = func(*args, **kwargs)
[rank7]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank7]:     return self_._op(*args, **kwargs)
[rank7]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 7 has a total capacity of 79.11 GiB of which 44.88 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank11]: Traceback (most recent call last):
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank11]:     main(config)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank11]:     return f(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank11]:     pred = model(input_ids)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank11]:     result = forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank11]:     h = layer(h, self.freqs_cis)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank11]:     result = forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank11]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank11]:     return disable_fn(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank11]:     return fn(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank11]:     ret = function(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank11]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank11]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank11]:     out = func(*args, **kwargs)
[rank11]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank11]:     return self_._op(*args, **kwargs)
[rank11]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank10]: Traceback (most recent call last):
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank10]:     main(config)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank10]:     return f(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank10]:     pred = model(input_ids)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank10]:     result = forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank10]:     h = layer(h, self.freqs_cis)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank10]:     result = forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank10]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank10]:     return disable_fn(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank10]:     return fn(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank10]:     ret = function(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank10]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank10]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank10]:     out = func(*args, **kwargs)
[rank10]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank10]:     return self_._op(*args, **kwargs)
[rank10]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 2 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank13]: Traceback (most recent call last):
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank13]:     main(config)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank13]:     return f(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank13]:     pred = model(input_ids)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank13]:     result = forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank13]:     h = layer(h, self.freqs_cis)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank13]:     result = forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank13]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank13]:     return disable_fn(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank13]:     return fn(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank13]:     ret = function(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank13]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank13]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank13]:     out = func(*args, **kwargs)
[rank13]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank13]:     return self_._op(*args, **kwargs)
[rank13]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 5 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank12]: Traceback (most recent call last):
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank12]:     main(config)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank12]:     return f(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank12]:     pred = model(input_ids)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank12]:     result = forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank12]:     h = layer(h, self.freqs_cis)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank12]:     result = forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank12]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank12]:     return disable_fn(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank12]:     return fn(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank12]:     ret = function(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank12]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank12]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank12]:     out = func(*args, **kwargs)
[rank12]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank12]:     return self_._op(*args, **kwargs)
[rank12]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 4 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank15]:     main(config)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank15]:     return f(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank15]:     pred = model(input_ids)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank15]:     result = forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank15]:     h = layer(h, self.freqs_cis)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank15]:     result = forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank15]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank15]:     return disable_fn(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank15]:     return fn(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank15]:     ret = function(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank15]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank15]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank15]:     return torch._C._nn.silu(input)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank15]:     out = func(*args, **kwargs)
[rank15]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank15]:     return self_._op(*args, **kwargs)
[rank15]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 7 has a total capacity of 79.11 GiB of which 60.88 MiB is free. Including non-PyTorch memory, this process has 79.03 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank9]: Traceback (most recent call last):
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank9]:     main(config)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank9]:     return f(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank9]:     pred = model(input_ids)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank9]:     result = forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank9]:     h = layer(h, self.freqs_cis)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank9]:     result = forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank9]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank9]:     return disable_fn(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank9]:     return fn(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank9]:     ret = function(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank9]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank9]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank9]:     out = func(*args, **kwargs)
[rank9]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank9]:     return self_._op(*args, **kwargs)
[rank9]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacity of 79.11 GiB of which 24.88 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank14]: Traceback (most recent call last):
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 503, in <module>
[rank14]:     main(config)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
[rank14]:     return f(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
[rank14]:     pred = model(input_ids)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank14]:     result = forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
[rank14]:     h = layer(h, self.freqs_cis)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
[rank14]:     result = forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
[rank14]:     return self.checkpoint_fn(  # type: ignore[misc]
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
[rank14]:     return disable_fn(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
[rank14]:     return fn(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
[rank14]:     ret = function(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
[rank14]:     out = h + self.feed_forward(self.ffn_norm(h))
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
[rank14]:     return self.w2(F.silu(self.w1(x)) * self.w3(x))
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
[rank14]:     return torch._C._nn.silu(input)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
[rank14]:     out = func(*args, **kwargs)
[rank14]:   File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
[rank14]:     return self_._op(*args, **kwargs)
[rank14]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 44.88 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129260 closing signal SIGTERM
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129261 closing signal SIGTERM
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129262 closing signal SIGTERM
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129263 closing signal SIGTERM
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129264 closing signal SIGTERM
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129265 closing signal SIGTERM
W0722 22:51:16.171000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 129267 closing signal SIGTERM
W0722 22:51:16.472000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083848 closing signal SIGTERM
W0722 22:51:16.477000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083849 closing signal SIGTERM
W0722 22:51:16.477000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083850 closing signal SIGTERM
W0722 22:51:16.477000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083852 closing signal SIGTERM
W0722 22:51:16.477000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083853 closing signal SIGTERM
W0722 22:51:16.478000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083854 closing signal SIGTERM
W0722 22:51:16.478000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:891] Sending process 4083855 closing signal SIGTERM
E0722 22:51:18.214000 22462552614720 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 6 (pid: 129266) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 22:51:18.222000 22462552614720 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_jpc50_5f/none_dk_jxl32/attempt_0/6/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_22:51:14
  host      : p4-r24-n1.bluevela.rmf.ibm.com
  rank      : 14 (local_rank: 6)
  exitcode  : 1 (pid: 129266)
  error_file: /tmp/torchelastic_jpc50_5f/none_dk_jxl32/attempt_0/6/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/functional.py", line 2380, in silu
      return torch._C._nn.silu(input)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 6 has a total capacity of 79.11 GiB of which 44.88 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 71.57 GiB is allocated by PyTorch, and 4.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================
E0722 22:51:18.520000 22892387923776 torch/distributed/elastic/multiprocessing/api.py:863] failed (exitcode: 1) local_rank: 3 (pid: 4083851) of binary: /proj/data-eng/lchu/miniconda3/envs/latest/bin/python
E0722 22:51:18.528000 22892387923776 torch/distributed/elastic/multiprocessing/errors/error_handler.py:141] no error file defined for parent, to copy child error file (/tmp/torchelastic_2prnd5v_/none_p6d3tbsp/attempt_0/3/error.json)
Traceback (most recent call last):
  File "/proj/data-eng/lchu/miniconda3/envs/latest/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-22_22:51:14
  host      : p4-r20-n4.bluevela.rmf.ibm.com
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 4083851)
  error_file: /tmp/torchelastic_2prnd5v_/none_p6d3tbsp/attempt_0/3/error.json
  traceback : Traceback (most recent call last):
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
      return f(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/train.py", line 383, in main
      pred = model(input_ids)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 442, in forward
      h = layer(h, self.freqs_cis)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1768, in _call_impl
      result = forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py", line 172, in forward
      return self.checkpoint_fn(  # type: ignore[misc]
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_compile.py", line 31, in inner
      return disable_fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 602, in _fn
      return fn(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 493, in checkpoint
      ret = function(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 332, in forward
      out = h + self.feed_forward(self.ffn_norm(h))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1716, in _wrapped_call_impl
      return self._call_impl(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1727, in _call_impl
      return forward_call(*args, **kwargs)
    File "/proj/data-eng/lchu/torchtitan/torchtitan/models/llama/model.py", line 258, in forward
      return self.w2(F.silu(self.w1(x)) * self.w3(x))
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/utils/checkpoint.py", line 1289, in __torch_dispatch__
      out = func(*args, **kwargs)
    File "/proj/data-eng/lchu/miniconda3/envs/latest/lib/python3.10/site-packages/torch/_ops.py", line 671, in __call__
      return self_._op(*args, **kwargs)
  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 3 has a total capacity of 79.11 GiB of which 8.88 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 71.38 GiB is allocated by PyTorch, and 4.78 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
  
============================================================

------------------------------------------------------------
Sender: LSF System <lsfadmin@p4-r20-n4>
Subject: Job 107837: <lchu-titan-async> in cluster <blue_vela> Exited

Job <lchu-titan-async> was submitted from host <login1> by user <lchu> in cluster <blue_vela> at Mon Jul 22 22:50:16 2024
Job was executed on host(s) <1*p4-r20-n4>, in queue <normal>, as user <lchu> in cluster <blue_vela> at Mon Jul 22 22:50:16 2024
                            <1*p4-r24-n1>
</u/lchu> was used as the home directory.
</proj/data-eng/lchu/torchtitan> was used as the working directory.
Started at Mon Jul 22 22:50:16 2024
Terminated at Mon Jul 22 22:51:20 2024
Results reported at Mon Jul 22 22:51:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
blaunch bash -c '\
torchrun \
    --nnodes=$num_nodes \
    --node_rank=`echo $(($LSF_PM_TASKID - 1))`  \
    --nproc_per_node=8  \
    --master-addr=`echo $LSB_HOSTS | xargs -n 1 | head -n 1` \
    --master-port=12234 \
    train.py $MODEL_ARGS
'
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   478.00 sec.
    Max Memory :                                 10087 MB
    Average Memory :                             4006.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                9
    Run time :                                   64 sec.
    Turnaround time :                            64 sec.

The output (if any) is above this job summary.

